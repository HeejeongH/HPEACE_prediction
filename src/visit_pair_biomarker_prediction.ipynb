{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ì›ë³¸ ë°ì´í„°: 29,098ê±´, 11,238ëª…\n",
      "\n",
      "ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„±:\n",
      "  ê¸°ë³¸ì •ë³´: 11ê°œ - ['ë‚˜ì´', 'ì„±ë³„', 'ì‹ ì¥', 'ì²´ì¤‘', 'ê³ í˜ˆì••_í†µí•©', 'ë‹¹ë‡¨_í†µí•©', 'ê³ ì§€í˜ˆì¦_í†µí•©', 'ë¹„ë§Œ', 'ì¼ë°˜ë‹´ë°°_í¡ì—°ì—¬ë¶€', 'ìŒì£¼', 'í™œë™ëŸ‰']\n",
      "  ì‹ìŠµê´€: 19ê°œ\n",
      "  ë°”ì´ì˜¤ë§ˆì»¤: 11ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì„¤ì •\n",
    "df = pd.read_excel('../data/total_again.xlsx')\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "print(f\"ğŸ“Š ì›ë³¸ ë°ì´í„°: {len(df):,}ê±´, {len(df['R-ID'].unique()):,}ëª…\")\n",
    "\n",
    "# ì»¬ëŸ¼ ì •ì˜\n",
    "id_col = 'R-ID'\n",
    "date_col = 'ìˆ˜ì§„ì¼'\n",
    "\n",
    "# ê¸°ë³¸ ì •ë³´\n",
    "baseline_features = {\n",
    "    'demographics': ['ë‚˜ì´', 'ì„±ë³„', 'ì‹ ì¥', 'ì²´ì¤‘'],\n",
    "    'diseases': ['ê³ í˜ˆì••_í†µí•©', 'ë‹¹ë‡¨_í†µí•©', 'ê³ ì§€í˜ˆì¦_í†µí•©', 'ë¹„ë§Œ'],\n",
    "    'lifestyle': ['ì¼ë°˜ë‹´ë°°_í¡ì—°ì—¬ë¶€', 'ìŒì£¼', 'í™œë™ëŸ‰']\n",
    "}\n",
    "\n",
    "# ì‹ìŠµê´€ ë³€ìˆ˜\n",
    "diet_features = [\n",
    "    'ê°„ì‹ë¹ˆë„', 'ê³ ì§€ë°© ìœ¡ë¥˜', 'ê³¡ë¥˜', 'ê³¼ì¼', 'ë‹¨ë§›', 'ë‹¨ë°±ì§ˆë¥˜', 'ë¬¼', 'ë°¥ ì–‘',\n",
    "    'ì‹ì‚¬ ë¹ˆë„', 'ì‹ì‚¬ëŸ‰', 'ì™¸ì‹ë¹ˆë„', 'ìœ ì œí’ˆ', 'ìŒë£Œë¥˜', 'ì¸ìŠ¤í„´íŠ¸ ê°€ê³µì‹í’ˆ',\n",
    "    'ì§  ê°„', 'ì§  ì‹ìŠµê´€', 'ì±„ì†Œ', 'ì»¤í”¼', 'íŠ€ê¹€'\n",
    "]\n",
    "\n",
    "# ë°”ì´ì˜¤ë§ˆì»¤\n",
    "biomarkers = [\n",
    "    'SBP', 'DBP', 'CHOL.', 'TG', 'LDL CHOL.', 'HDL CHOL.', \n",
    "    'GLUCOSE', 'HBA1C', 'eGFR', 'í—ˆë¦¬ë‘˜ë ˆ(WAIST)', 'ì²´ì§ˆëŸ‰ì§€ìˆ˜'\n",
    "]\n",
    "\n",
    "# ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "all_baseline = []\n",
    "for category, features in baseline_features.items():\n",
    "    all_baseline.extend([f for f in features if f in df.columns])\n",
    "\n",
    "available_diet = [f for f in diet_features if f in df.columns]\n",
    "available_biomarkers = [f for f in biomarkers if f in df.columns]\n",
    "\n",
    "print(f\"\\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„±:\")\n",
    "print(f\"  ê¸°ë³¸ì •ë³´: {len(all_baseline)}ê°œ - {all_baseline}\")\n",
    "print(f\"  ì‹ìŠµê´€: {len(available_diet)}ê°œ\")\n",
    "print(f\"  ë°”ì´ì˜¤ë§ˆì»¤: {len(available_biomarkers)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬...\n",
      "  ì „ì²˜ë¦¬ ì™„ë£Œ: 29,098ê±´\n",
      "  ë‹¤ì¤‘ ë°©ë¬¸ í™˜ì: 11,238ëª…\n",
      "  í‰ê·  ë°©ë¬¸ íšŸìˆ˜: 2.59íšŒ\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬\n",
    "print(\"ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬...\")\n",
    "\n",
    "# í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "required_cols = [id_col, date_col] + all_baseline + available_diet + available_biomarkers\n",
    "df_clean = df[required_cols].copy()\n",
    "\n",
    "# ë‚ ì§œ ë³€í™˜\n",
    "df_clean[date_col] = pd.to_datetime(df_clean[date_col], errors='coerce')\n",
    "df_clean = df_clean.dropna(subset=[date_col])\n",
    "\n",
    "# ì •ë ¬\n",
    "df_clean = df_clean.sort_values([id_col, date_col]).reset_index(drop=True)\n",
    "\n",
    "# 2íšŒ ì´ìƒ ë°©ë¬¸í•œ í™˜ìë§Œ\n",
    "visit_counts = df_clean.groupby(id_col).size()\n",
    "multi_visit_patients = visit_counts[visit_counts >= 2].index\n",
    "df_multi = df_clean[df_clean[id_col].isin(multi_visit_patients)].copy()\n",
    "\n",
    "print(f\"  ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df_multi):,}ê±´\")\n",
    "print(f\"  ë‹¤ì¤‘ ë°©ë¬¸ í™˜ì: {len(multi_visit_patients):,}ëª…\")\n",
    "print(f\"  í‰ê·  ë°©ë¬¸ íšŸìˆ˜: {df_multi.groupby(id_col).size().mean():.2f}íšŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ì²«-ë§ˆì§€ë§‰ ë°©ë¬¸ ìŒ ìƒì„± (ê°„ê²©: 30~2190ì¼)\n",
      "ğŸ”„ ìµœëŒ€ ë³€í™” ë°©ë¬¸ ìŒ ìƒì„± (ê°„ê²©: 30~2190ì¼)\n",
      "\n",
      "ğŸ“Š ë°©ë¬¸ ìŒ ìƒì„± ê²°ê³¼:\n",
      "  ì²«-ë§ˆì§€ë§‰ ì „ëµ: 10,151ê°œ ìŒ\n",
      "  ìµœëŒ€ë³€í™” ì „ëµ: 10,947ê°œ ìŒ\n",
      "  ì²«-ë§ˆì§€ë§‰ í‰ê·  ê°„ê²©: 950.6ì¼\n",
      "  ìµœëŒ€ë³€í™” í‰ê·  ê°„ê²©: 839.6ì¼\n",
      "  í‰ê·  ë³€í™”ì ìˆ˜: 0.1226\n"
     ]
    }
   ],
   "source": [
    "def create_first_last_pairs(df, id_col, date_col, min_interval_days=30, max_interval_days=2190):\n",
    "    \"\"\"\n",
    "    ì²« ë°©ë¬¸ - ë§ˆì§€ë§‰ ë°©ë¬¸ ìŒ ìƒì„±\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”„ ì²«-ë§ˆì§€ë§‰ ë°©ë¬¸ ìŒ ìƒì„± (ê°„ê²©: {min_interval_days}~{max_interval_days}ì¼)\")\n",
    "\n",
    "    pairs = []\n",
    "\n",
    "    for patient_id in df[id_col].unique():\n",
    "        patient_data = df[df[id_col] == patient_id].sort_values(date_col)\n",
    "\n",
    "        if len(patient_data) >= 2:\n",
    "            first_visit = patient_data.iloc[0]\n",
    "            last_visit = patient_data.iloc[-1]\n",
    "\n",
    "            # ë°©ë¬¸ ê°„ê²© ê³„ì‚°\n",
    "            days_interval = (last_visit[date_col] - first_visit[date_col]).days\n",
    "\n",
    "            if min_interval_days <= days_interval <= max_interval_days:\n",
    "                pairs.append({\n",
    "                    'patient_id': patient_id,\n",
    "                    'first_visit': first_visit,\n",
    "                    'second_visit': last_visit,\n",
    "                    'days_interval': days_interval,\n",
    "                    'visit_gap': len(patient_data) - 1,  # ì¤‘ê°„ì— ëª‡ ë²ˆì˜ ë°©ë¬¸ì´ ìˆì—ˆëŠ”ì§€\n",
    "                    'strategy': 'first_last'\n",
    "                })\n",
    "\n",
    "    return pairs\n",
    "\n",
    "def create_max_change_pairs(df, id_col, date_col, biomarkers, min_interval_days=30, max_interval_days=2190):\n",
    "    \"\"\"\n",
    "    ë°”ì´ì˜¤ë§ˆì»¤ ë³€í™”ê°€ ê°€ì¥ í° ë°©ë¬¸ ìŒ ìƒì„±\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”„ ìµœëŒ€ ë³€í™” ë°©ë¬¸ ìŒ ìƒì„± (ê°„ê²©: {min_interval_days}~{max_interval_days}ì¼)\")\n",
    "\n",
    "    pairs = []\n",
    "\n",
    "    for patient_id in df[id_col].unique():\n",
    "        patient_data = df[df[id_col] == patient_id].sort_values(date_col)\n",
    "\n",
    "        if len(patient_data) >= 2:\n",
    "            max_change_score = 0\n",
    "            best_pair = None\n",
    "\n",
    "            # ëª¨ë“  ë°©ë¬¸ ìŒ ì¡°í•© í™•ì¸\n",
    "            for i in range(len(patient_data)):\n",
    "                for j in range(i+1, len(patient_data)):\n",
    "                    visit1 = patient_data.iloc[i]\n",
    "                    visit2 = patient_data.iloc[j]\n",
    "\n",
    "                    days_interval = (visit2[date_col] - visit1[date_col]).days\n",
    "\n",
    "                    if min_interval_days <= days_interval <= max_interval_days:\n",
    "                        # ë°”ì´ì˜¤ë§ˆì»¤ ë³€í™”ëŸ‰ ê³„ì‚°\n",
    "                        change_score = 0\n",
    "                        valid_changes = 0\n",
    "\n",
    "                        for biomarker in biomarkers:\n",
    "                            if biomarker in visit1.index and biomarker in visit2.index:\n",
    "                                val1 = pd.to_numeric(visit1[biomarker], errors='coerce')\n",
    "                                val2 = pd.to_numeric(visit2[biomarker], errors='coerce')\n",
    "\n",
    "                                if pd.notna(val1) and pd.notna(val2) and val1 > 0:\n",
    "                                    # ìƒëŒ€ì  ë³€í™”ëŸ‰ ì‚¬ìš©\n",
    "                                    relative_change = abs(val2 - val1) / val1\n",
    "                                    change_score += relative_change\n",
    "                                    valid_changes += 1\n",
    "\n",
    "                        if valid_changes > 0:\n",
    "                            avg_change_score = change_score / valid_changes\n",
    "\n",
    "                            if avg_change_score > max_change_score:\n",
    "                                max_change_score = avg_change_score\n",
    "                                best_pair = {\n",
    "                                    'patient_id': patient_id,\n",
    "                                    'first_visit': visit1,\n",
    "                                    'second_visit': visit2,\n",
    "                                    'days_interval': days_interval,\n",
    "                                    'change_score': avg_change_score,\n",
    "                                    'valid_biomarkers': valid_changes,\n",
    "                                    'strategy': 'max_change'\n",
    "                                }\n",
    "\n",
    "            if best_pair is not None:\n",
    "                pairs.append(best_pair)\n",
    "\n",
    "    return pairs\n",
    "\n",
    "# ========== ì—¬ê¸°ê°€ ì¤‘ìš”! í•¨ìˆ˜ í˜¸ì¶œ ì‹œ ì¸ì ìˆ˜ì • ==========\n",
    "# ë‘ ì „ëµìœ¼ë¡œ ë°©ë¬¸ ìŒ ìƒì„±\n",
    "first_last_pairs = create_first_last_pairs(df_multi, id_col, date_col)  # â† available_biomarkers ì œê±°!\n",
    "max_change_pairs = create_max_change_pairs(df_multi, id_col, date_col, available_biomarkers)\n",
    "\n",
    "print(f\"\\nğŸ“Š ë°©ë¬¸ ìŒ ìƒì„± ê²°ê³¼:\")\n",
    "print(f\"  ì²«-ë§ˆì§€ë§‰ ì „ëµ: {len(first_last_pairs):,}ê°œ ìŒ\")\n",
    "print(f\"  ìµœëŒ€ë³€í™” ì „ëµ: {len(max_change_pairs):,}ê°œ ìŒ\")\n",
    "\n",
    "if first_last_pairs:\n",
    "    fl_intervals = [p['days_interval'] for p in first_last_pairs]\n",
    "    print(f\"  ì²«-ë§ˆì§€ë§‰ í‰ê·  ê°„ê²©: {np.mean(fl_intervals):.1f}ì¼\")\n",
    "\n",
    "if max_change_pairs:\n",
    "    mc_intervals = [p['days_interval'] for p in max_change_pairs]\n",
    "    mc_changes = [p['change_score'] for p in max_change_pairs]\n",
    "    print(f\"  ìµœëŒ€ë³€í™” í‰ê·  ê°„ê²©: {np.mean(mc_intervals):.1f}ì¼\")\n",
    "    print(f\"  í‰ê·  ë³€í™”ì ìˆ˜: {np.mean(mc_changes):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ì²«-ë§ˆì§€ë§‰ ë°©ë¬¸ ì „ëµ ===\n",
      "ğŸ”„ ëª¨ë¸ë§ ë°ì´í„°ì…‹ ìƒì„± (10,151ê°œ ìŒ)...\n",
      "âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ:\n",
      "  ë ˆì½”ë“œ: 10,151ê°œ\n",
      "  íŠ¹ì„±: 98ê°œ\n",
      "\n",
      "=== ìµœëŒ€ ë³€í™” ì „ëµ ===\n",
      "ğŸ”„ ëª¨ë¸ë§ ë°ì´í„°ì…‹ ìƒì„± (10,947ê°œ ìŒ)...\n",
      "âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ:\n",
      "  ë ˆì½”ë“œ: 10,947ê°œ\n",
      "  íŠ¹ì„±: 98ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ë§ ë°ì´í„°ì…‹ ìƒì„± í•¨ìˆ˜\n",
    "def create_modeling_dataset(pairs, baseline_features, diet_features, biomarkers):\n",
    "    \"\"\"\n",
    "    ë°©ë¬¸ ìŒìœ¼ë¡œë¶€í„° ëª¨ë¸ë§ ë°ì´í„°ì…‹ ìƒì„±\n",
    "    \n",
    "    Input features:\n",
    "    - ì²« ë°©ë¬¸ ê¸°ë³¸ì •ë³´ (ë‚˜ì´, ì„±ë³„, ì‹ ì¥, ì§ˆë³‘ìœ ë¬´ ë“±)\n",
    "    - ì‹ìŠµê´€ ë³€í™”ëŸ‰ (ë‘ ë²ˆì§¸ - ì²« ë²ˆì§¸)\n",
    "    \n",
    "    Output:\n",
    "    - ë°”ì´ì˜¤ë§ˆì»¤ ë³€í™”ëŸ‰\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ğŸ”„ ëª¨ë¸ë§ ë°ì´í„°ì…‹ ìƒì„± ({len(pairs):,}ê°œ ìŒ)...\")\n",
    "    \n",
    "    modeling_data = []\n",
    "    \n",
    "    for pair in pairs:\n",
    "        first = pair['first_visit']\n",
    "        second = pair['second_visit']\n",
    "        \n",
    "        row = {\n",
    "            'patient_id': pair['patient_id'],\n",
    "            'days_interval': pair['days_interval'],\n",
    "            'interval_years': pair['days_interval'] / 365.25,\n",
    "            'strategy': pair['strategy']\n",
    "        }\n",
    "        \n",
    "        # ì¶”ê°€ ì •ë³´ (ì „ëµë³„)\n",
    "        if 'visit_gap' in pair:\n",
    "            row['visit_gap'] = pair['visit_gap']\n",
    "        if 'change_score' in pair:\n",
    "            row['change_score'] = pair['change_score']\n",
    "            \n",
    "        # 1. ì²« ë°©ë¬¸ ê¸°ë³¸ì •ë³´\n",
    "        for feature in baseline_features:\n",
    "            if feature in first.index:\n",
    "                value = first[feature]\n",
    "                \n",
    "                # íŠ¹ë³„ ì²˜ë¦¬\n",
    "                if feature == 'ì„±ë³„':\n",
    "                    row[f'baseline_{feature}'] = 1 if value == 'M' else 0\n",
    "                elif feature == 'ì¼ë°˜ë‹´ë°°_í¡ì—°ì—¬ë¶€':\n",
    "                    row[f'baseline_{feature}'] = 1 if value == 'Y' else 0\n",
    "                elif feature == 'ìŒì£¼':\n",
    "                    row[f'baseline_{feature}'] = pd.to_numeric(value, errors='coerce')\n",
    "                elif feature in ['ê³ í˜ˆì••_í†µí•©', 'ë‹¹ë‡¨_í†µí•©', 'ê³ ì§€í˜ˆì¦_í†µí•©', 'ë¹„ë§Œ']:\n",
    "                    row[f'baseline_{feature}'] = 1 if value == 'Y' else 0\n",
    "                else:\n",
    "                    row[f'baseline_{feature}'] = pd.to_numeric(value, errors='coerce')\n",
    "        \n",
    "        # 2. ì‹ìŠµê´€ ë³€í™”ëŸ‰ (í•µì‹¬ ì˜ˆì¸¡ íŠ¹ì„±)\n",
    "        for feature in diet_features:\n",
    "            if feature in first.index and feature in second.index:\n",
    "                val1 = pd.to_numeric(first[feature], errors='coerce')\n",
    "                val2 = pd.to_numeric(second[feature], errors='coerce')\n",
    "                \n",
    "                if pd.notna(val1) and pd.notna(val2):\n",
    "                    # ì ˆëŒ€ ë³€í™”ëŸ‰\n",
    "                    row[f'diet_change_{feature}'] = val2 - val1\n",
    "                    \n",
    "                    # ìƒëŒ€ ë³€í™”ìœ¨ (val1ì´ 0ì´ ì•„ë‹Œ ê²½ìš°)\n",
    "                    if val1 != 0:\n",
    "                        row[f'diet_rel_change_{feature}'] = (val2 - val1) / abs(val1)\n",
    "                    else:\n",
    "                        row[f'diet_rel_change_{feature}'] = 0\n",
    "                else:\n",
    "                    row[f'diet_change_{feature}'] = np.nan\n",
    "                    row[f'diet_rel_change_{feature}'] = np.nan\n",
    "        \n",
    "        # 3. ì²« ë°©ë¬¸ ì‹ìŠµê´€ ìƒíƒœ (baseline)\n",
    "        for feature in diet_features:\n",
    "            if feature in first.index:\n",
    "                row[f'baseline_diet_{feature}'] = pd.to_numeric(first[feature], errors='coerce')\n",
    "        \n",
    "        # 4. ë³µí•© ì‹ìŠµê´€ ì§€í‘œ\n",
    "        # ê±´ê°•í•œ ì‹ìŠµê´€ ë³€í™”\n",
    "        healthy_foods = ['ê³¼ì¼', 'ì±„ì†Œ', 'ë‹¨ë°±ì§ˆë¥˜', 'ë¬¼', 'ìœ ì œí’ˆ']\n",
    "        healthy_changes = []\n",
    "        for food in healthy_foods:\n",
    "            if f'diet_change_{food}' in row and pd.notna(row[f'diet_change_{food}']):\n",
    "                healthy_changes.append(row[f'diet_change_{food}'])\n",
    "        \n",
    "        if healthy_changes:\n",
    "            row['healthy_diet_change_score'] = np.mean(healthy_changes)\n",
    "        else:\n",
    "            row['healthy_diet_change_score'] = 0\n",
    "        \n",
    "        # ë¶ˆê±´ê°•í•œ ì‹ìŠµê´€ ë³€í™”\n",
    "        unhealthy_foods = ['ë‹¨ë§›', 'íŠ€ê¹€', 'ì¸ìŠ¤í„´íŠ¸ ê°€ê³µì‹í’ˆ', 'ì™¸ì‹ë¹ˆë„', 'ìŒë£Œë¥˜']\n",
    "        unhealthy_changes = []\n",
    "        for food in unhealthy_foods:\n",
    "            if f'diet_change_{food}' in row and pd.notna(row[f'diet_change_{food}']):\n",
    "                unhealthy_changes.append(row[f'diet_change_{food}'])\n",
    "        \n",
    "        if unhealthy_changes:\n",
    "            row['unhealthy_diet_change_score'] = np.mean(unhealthy_changes)\n",
    "        else:\n",
    "            row['unhealthy_diet_change_score'] = 0\n",
    "        \n",
    "        # ì „ì²´ ì‹ìŠµê´€ ê°œì„  ì ìˆ˜\n",
    "        row['diet_improvement_score'] = row['healthy_diet_change_score'] - row['unhealthy_diet_change_score']\n",
    "        \n",
    "        # 5. íƒ€ê²Ÿ: ë°”ì´ì˜¤ë§ˆì»¤ ë³€í™”ëŸ‰\n",
    "        for biomarker in biomarkers:\n",
    "            if biomarker in first.index and biomarker in second.index:\n",
    "                val1 = pd.to_numeric(first[biomarker], errors='coerce')\n",
    "                val2 = pd.to_numeric(second[biomarker], errors='coerce')\n",
    "                \n",
    "                if pd.notna(val1) and pd.notna(val2) and val1 > 0 and val2 > 0:\n",
    "                    # ì ˆëŒ€ ë³€í™”ëŸ‰\n",
    "                    row[f'target_{biomarker}'] = val2 - val1\n",
    "                    \n",
    "                    # ìƒëŒ€ ë³€í™”ìœ¨\n",
    "                    row[f'target_rel_{biomarker}'] = (val2 - val1) / val1\n",
    "                else:\n",
    "                    row[f'target_{biomarker}'] = np.nan\n",
    "                    row[f'target_rel_{biomarker}'] = np.nan\n",
    "        \n",
    "        modeling_data.append(row)\n",
    "    \n",
    "    dataset = pd.DataFrame(modeling_data)\n",
    "    \n",
    "    print(f\"âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ:\")\n",
    "    print(f\"  ë ˆì½”ë“œ: {len(dataset):,}ê°œ\")\n",
    "    print(f\"  íŠ¹ì„±: {len(dataset.columns)}ê°œ\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# ë‘ ì „ëµë³„ë¡œ ë°ì´í„°ì…‹ ìƒì„±\n",
    "print(\"\\n=== ì²«-ë§ˆì§€ë§‰ ë°©ë¬¸ ì „ëµ ===\")\n",
    "first_last_dataset = create_modeling_dataset(first_last_pairs, all_baseline, available_diet, available_biomarkers)\n",
    "\n",
    "print(\"\\n=== ìµœëŒ€ ë³€í™” ì „ëµ ===\")\n",
    "max_change_dataset = create_modeling_dataset(max_change_pairs, all_baseline, available_diet, available_biomarkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ì²«-ë§ˆì§€ë§‰ ì „ëµ ë°ì´í„°ì…‹ í’ˆì§ˆ ë¶„ì„:\n",
      "  ì´ ë ˆì½”ë“œ: 10,151ê°œ\n",
      "\n",
      "ğŸ”¬ ë°”ì´ì˜¤ë§ˆì»¤ ë³€í™”ëŸ‰ ë¶„ì„:\n",
      "    âœ… SBP: 10,151ê°œ, í‰ê· ë³€í™”=+0.068Â±12.287\n",
      "       í° ë³€í™” (>1SD): 2784ê°œ (27.4%)\n",
      "    âœ… DBP: 10,151ê°œ, í‰ê· ë³€í™”=-0.634Â±8.814\n",
      "       í° ë³€í™” (>1SD): 3115ê°œ (30.7%)\n",
      "    âœ… CHOL.: 10,151ê°œ, í‰ê· ë³€í™”=+2.346Â±35.046\n",
      "       í° ë³€í™” (>1SD): 2471ê°œ (24.3%)\n",
      "    âœ… TG: 10,151ê°œ, í‰ê· ë³€í™”=-1.430Â±60.988\n",
      "       í° ë³€í™” (>1SD): 1661ê°œ (16.4%)\n",
      "    âœ… LDL CHOL.: 10,151ê°œ, í‰ê· ë³€í™”=-0.530Â±31.217\n",
      "       í° ë³€í™” (>1SD): 2333ê°œ (23.0%)\n",
      "    âœ… HDL CHOL.: 10,151ê°œ, í‰ê· ë³€í™”=+0.844Â±8.010\n",
      "       í° ë³€í™” (>1SD): 2482ê°œ (24.5%)\n",
      "    âœ… GLUCOSE: 10,151ê°œ, í‰ê· ë³€í™”=+0.476Â±13.579\n",
      "       í° ë³€í™” (>1SD): 1440ê°œ (14.2%)\n",
      "    âœ… HBA1C: 10,151ê°œ, í‰ê· ë³€í™”=-0.045Â±0.379\n",
      "       í° ë³€í™” (>1SD): 1237ê°œ (12.2%)\n",
      "    âœ… eGFR: 10,151ê°œ, í‰ê· ë³€í™”=-1.430Â±10.251\n",
      "       í° ë³€í™” (>1SD): 3009ê°œ (29.6%)\n",
      "    âœ… í—ˆë¦¬ë‘˜ë ˆ(WAIST): 10,151ê°œ, í‰ê· ë³€í™”=+0.366Â±3.939\n",
      "       í° ë³€í™” (>1SD): 2856ê°œ (28.1%)\n",
      "    âœ… ì²´ì§ˆëŸ‰ì§€ìˆ˜: 10,151ê°œ, í‰ê· ë³€í™”=+0.066Â±1.121\n",
      "       í° ë³€í™” (>1SD): 2329ê°œ (22.9%)\n",
      "\n",
      "ğŸ½ï¸ ì‹ìŠµê´€ ë³€í™” ë¶„ì„:\n",
      "  ì‹ìŠµê´€ ë³€í™” íŠ¹ì„±: 19ê°œ\n",
      "    ê°„ì‹ë¹ˆë„: í‰ê· =-0.030Â±0.695, ë³€í™”ìˆìŒ=3,958ê°œ\n",
      "    ê³ ì§€ë°© ìœ¡ë¥˜: í‰ê· =+0.009Â±0.647, ë³€í™”ìˆìŒ=3,219ê°œ\n",
      "    ê³¡ë¥˜: í‰ê· =-0.078Â±0.749, ë³€í™”ìˆìŒ=4,059ê°œ\n",
      "    ê³¼ì¼: í‰ê· =-0.004Â±0.679, ë³€í™”ìˆìŒ=3,636ê°œ\n",
      "    ë‹¨ë§›: í‰ê· =-0.028Â±0.538, ë³€í™”ìˆìŒ=2,817ê°œ\n",
      "    ë‹¨ë°±ì§ˆë¥˜: í‰ê· =+0.023Â±0.825, ë³€í™”ìˆìŒ=4,790ê°œ\n",
      "    ë¬¼: í‰ê· =+0.027Â±0.739, ë³€í™”ìˆìŒ=4,220ê°œ\n",
      "    ë°¥ ì–‘: í‰ê· =-0.057Â±0.496, ë³€í™”ìˆìŒ=2,380ê°œ\n",
      "    ì‹ì‚¬ ë¹ˆë„: í‰ê· =-0.005Â±0.774, ë³€í™”ìˆìŒ=2,562ê°œ\n",
      "    ì‹ì‚¬ëŸ‰: í‰ê· =-0.039Â±0.466, ë³€í™”ìˆìŒ=2,116ê°œ\n",
      "\n",
      "ğŸ“ˆ ë³µí•© ì‹ìŠµê´€ ì§€í‘œ:\n",
      "    healthy_diet_change_score: í‰ê· =+0.032Â±0.420\n",
      "    unhealthy_diet_change_score: í‰ê· =-0.028Â±0.330\n",
      "    diet_improvement_score: í‰ê· =+0.060Â±0.532\n",
      "\n",
      "ğŸ“Š ìµœëŒ€ë³€í™” ì „ëµ ë°ì´í„°ì…‹ í’ˆì§ˆ ë¶„ì„:\n",
      "  ì´ ë ˆì½”ë“œ: 10,947ê°œ\n",
      "\n",
      "ğŸ”¬ ë°”ì´ì˜¤ë§ˆì»¤ ë³€í™”ëŸ‰ ë¶„ì„:\n",
      "    âœ… SBP: 10,947ê°œ, í‰ê· ë³€í™”=+0.363Â±12.887\n",
      "       í° ë³€í™” (>1SD): 3276ê°œ (29.9%)\n",
      "    âœ… DBP: 10,947ê°œ, í‰ê· ë³€í™”=-0.325Â±9.288\n",
      "       í° ë³€í™” (>1SD): 3125ê°œ (28.5%)\n",
      "    âœ… CHOL.: 10,947ê°œ, í‰ê· ë³€í™”=+4.224Â±37.696\n",
      "       í° ë³€í™” (>1SD): 2859ê°œ (26.1%)\n",
      "    âœ… TG: 10,947ê°œ, í‰ê· ë³€í™”=+3.365Â±68.822\n",
      "       í° ë³€í™” (>1SD): 1893ê°œ (17.3%)\n",
      "    âœ… LDL CHOL.: 10,947ê°œ, í‰ê· ë³€í™”=+0.876Â±33.541\n",
      "       í° ë³€í™” (>1SD): 2662ê°œ (24.3%)\n",
      "    âœ… HDL CHOL.: 10,947ê°œ, í‰ê· ë³€í™”=+0.886Â±8.430\n",
      "       í° ë³€í™” (>1SD): 2966ê°œ (27.1%)\n",
      "    âœ… GLUCOSE: 10,947ê°œ, í‰ê· ë³€í™”=+0.800Â±14.503\n",
      "       í° ë³€í™” (>1SD): 1512ê°œ (13.8%)\n",
      "    âœ… HBA1C: 10,947ê°œ, í‰ê· ë³€í™”=-0.033Â±0.397\n",
      "       í° ë³€í™” (>1SD): 1360ê°œ (12.4%)\n",
      "    âœ… eGFR: 10,947ê°œ, í‰ê· ë³€í™”=-1.198Â±10.501\n",
      "       í° ë³€í™” (>1SD): 3236ê°œ (29.6%)\n",
      "    âœ… í—ˆë¦¬ë‘˜ë ˆ(WAIST): 10,947ê°œ, í‰ê· ë³€í™”=+0.413Â±4.004\n",
      "       í° ë³€í™” (>1SD): 2611ê°œ (23.9%)\n",
      "    âœ… ì²´ì§ˆëŸ‰ì§€ìˆ˜: 10,947ê°œ, í‰ê· ë³€í™”=+0.087Â±1.137\n",
      "       í° ë³€í™” (>1SD): 2524ê°œ (23.1%)\n",
      "\n",
      "ğŸ½ï¸ ì‹ìŠµê´€ ë³€í™” ë¶„ì„:\n",
      "  ì‹ìŠµê´€ ë³€í™” íŠ¹ì„±: 19ê°œ\n",
      "    ê°„ì‹ë¹ˆë„: í‰ê· =-0.029Â±0.696, ë³€í™”ìˆìŒ=4,278ê°œ\n",
      "    ê³ ì§€ë°© ìœ¡ë¥˜: í‰ê· =+0.015Â±0.644, ë³€í™”ìˆìŒ=3,467ê°œ\n",
      "    ê³¡ë¥˜: í‰ê· =-0.064Â±0.750, ë³€í™”ìˆìŒ=4,343ê°œ\n",
      "    ê³¼ì¼: í‰ê· =-0.007Â±0.672, ë³€í™”ìˆìŒ=3,873ê°œ\n",
      "    ë‹¨ë§›: í‰ê· =-0.021Â±0.537, ë³€í™”ìˆìŒ=3,019ê°œ\n",
      "    ë‹¨ë°±ì§ˆë¥˜: í‰ê· =+0.022Â±0.825, ë³€í™”ìˆìŒ=5,148ê°œ\n",
      "    ë¬¼: í‰ê· =+0.025Â±0.726, ë³€í™”ìˆìŒ=4,455ê°œ\n",
      "    ë°¥ ì–‘: í‰ê· =-0.045Â±0.492, ë³€í™”ìˆìŒ=2,515ê°œ\n",
      "    ì‹ì‚¬ ë¹ˆë„: í‰ê· =-0.004Â±0.763, ë³€í™”ìˆìŒ=2,708ê°œ\n",
      "    ì‹ì‚¬ëŸ‰: í‰ê· =-0.034Â±0.461, ë³€í™”ìˆìŒ=2,227ê°œ\n",
      "\n",
      "ğŸ“ˆ ë³µí•© ì‹ìŠµê´€ ì§€í‘œ:\n",
      "    healthy_diet_change_score: í‰ê· =+0.027Â±0.414\n",
      "    unhealthy_diet_change_score: í‰ê· =-0.023Â±0.325\n",
      "    diet_improvement_score: í‰ê· =+0.051Â±0.524\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°ì…‹ í’ˆì§ˆ ë¶„ì„\n",
    "def analyze_dataset_quality(dataset, strategy_name):\n",
    "    print(f\"\\nğŸ“Š {strategy_name} ë°ì´í„°ì…‹ í’ˆì§ˆ ë¶„ì„:\")\n",
    "    print(f\"  ì´ ë ˆì½”ë“œ: {len(dataset):,}ê°œ\")\n",
    "    \n",
    "    # íƒ€ê²Ÿ ë³€ìˆ˜ ìœ íš¨ì„± í™•ì¸\n",
    "    target_cols = [col for col in dataset.columns if col.startswith('target_') and not 'rel_' in col]\n",
    "    valid_targets = []\n",
    "    \n",
    "    print(f\"\\nğŸ”¬ ë°”ì´ì˜¤ë§ˆì»¤ ë³€í™”ëŸ‰ ë¶„ì„:\")\n",
    "    for col in target_cols:\n",
    "        valid_count = dataset[col].notna().sum()\n",
    "        if valid_count > 50:  # ìµœì†Œ 50ê°œ ì´ìƒ\n",
    "            valid_targets.append(col)\n",
    "            biomarker_name = col.replace('target_', '')\n",
    "            values = dataset[col].dropna()\n",
    "            mean_change = values.mean()\n",
    "            std_change = values.std()\n",
    "            print(f\"    âœ… {biomarker_name}: {valid_count:,}ê°œ, í‰ê· ë³€í™”={mean_change:+.3f}Â±{std_change:.3f}\")\n",
    "            \n",
    "            # í° ë³€í™” ë¹„ìœ¨\n",
    "            large_changes = (values.abs() > std_change).sum()\n",
    "            print(f\"       í° ë³€í™” (>1SD): {large_changes}ê°œ ({large_changes/len(values)*100:.1f}%)\")\n",
    "        else:\n",
    "            biomarker_name = col.replace('target_', '')\n",
    "            print(f\"    âŒ {biomarker_name}: {valid_count}ê°œ (ë¶€ì¡±)\")\n",
    "    \n",
    "    # ì‹ìŠµê´€ ë³€í™” ë¶„ì„\n",
    "    diet_change_cols = [col for col in dataset.columns if col.startswith('diet_change_')]\n",
    "    print(f\"\\nğŸ½ï¸ ì‹ìŠµê´€ ë³€í™” ë¶„ì„:\")\n",
    "    print(f\"  ì‹ìŠµê´€ ë³€í™” íŠ¹ì„±: {len(diet_change_cols)}ê°œ\")\n",
    "    \n",
    "    significant_changes = []\n",
    "    for col in diet_change_cols[:10]:  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ\n",
    "        values = dataset[col].dropna()\n",
    "        if len(values) > 0:\n",
    "            mean_change = values.mean()\n",
    "            std_change = values.std()\n",
    "            non_zero_count = (values.abs() > 0.01).sum()\n",
    "            \n",
    "            diet_name = col.replace('diet_change_', '')\n",
    "            print(f\"    {diet_name}: í‰ê· ={mean_change:+.3f}Â±{std_change:.3f}, ë³€í™”ìˆìŒ={non_zero_count:,}ê°œ\")\n",
    "            \n",
    "            if abs(mean_change) > 0.1 or std_change > 0.3:\n",
    "                significant_changes.append(col)\n",
    "    \n",
    "    # ë³µí•© ì§€í‘œ\n",
    "    print(f\"\\nğŸ“ˆ ë³µí•© ì‹ìŠµê´€ ì§€í‘œ:\")\n",
    "    composite_cols = ['healthy_diet_change_score', 'unhealthy_diet_change_score', 'diet_improvement_score']\n",
    "    \n",
    "    for col in composite_cols:\n",
    "        if col in dataset.columns:\n",
    "            values = dataset[col].dropna()\n",
    "            if len(values) > 0:\n",
    "                print(f\"    {col}: í‰ê· ={values.mean():+.3f}Â±{values.std():.3f}\")\n",
    "    \n",
    "    return valid_targets, significant_changes\n",
    "\n",
    "# ë‘ ì „ëµ ë°ì´í„°ì…‹ í’ˆì§ˆ ë¶„ì„\n",
    "fl_valid_targets, fl_significant_changes = analyze_dataset_quality(first_last_dataset, \"ì²«-ë§ˆì§€ë§‰ ì „ëµ\")\n",
    "mc_valid_targets, mc_significant_changes = analyze_dataset_quality(max_change_dataset, \"ìµœëŒ€ë³€í™” ì „ëµ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ëª¨ë¸ í›ˆë ¨ ì‹œì‘\n",
      "================================================================================\n",
      "\n",
      "ğŸš€ ì²«-ë§ˆì§€ë§‰ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨:\n",
      "  ì…ë ¥ íŠ¹ì„± êµ¬ì„±:\n",
      "    ê¸°ë³¸ì •ë³´: 30ê°œ\n",
      "    ì‹ìŠµê´€ë³€í™”: 38ê°œ\n",
      "    ë³µí•©ì§€í‘œ: 3ê°œ\n",
      "    ì „ëµíŠ¹ì„±: 1ê°œ\n",
      "    ì´ íŠ¹ì„±: 74ê°œ\n",
      "\n",
      "ğŸ”„ SBP ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 3,982ê°œ (ì›ë³¸ì˜ 100.0%)\n",
      "    XGBoost: RÂ²=-0.142, CV=-0.080Â±0.033, MAE=9.816\n",
      "    RandomForest: RÂ²=-0.029, CV=0.004Â±0.013, MAE=9.244\n",
      "    GradientBoosting: RÂ²=-0.119, CV=-0.050Â±0.030, MAE=9.693\n",
      "    ElasticNet: RÂ²=-0.006, CV=0.011Â±0.009, MAE=9.142\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=-999.000)\n",
      "\n",
      "ğŸ”„ DBP ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 3,982ê°œ (ì›ë³¸ì˜ 100.0%)\n",
      "    XGBoost: RÂ²=-0.125, CV=-0.091Â±0.029, MAE=7.002\n",
      "    RandomForest: RÂ²=-0.017, CV=-0.001Â±0.013, MAE=6.663\n",
      "    GradientBoosting: RÂ²=-0.096, CV=-0.076Â±0.035, MAE=6.908\n",
      "    ElasticNet: RÂ²=-0.007, CV=0.002Â±0.008, MAE=6.657\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=-999.000)\n",
      "\n",
      "ğŸ”„ CHOL. ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 3,982ê°œ (ì›ë³¸ì˜ 100.0%)\n",
      "    XGBoost: RÂ²=-0.117, CV=-0.112Â±0.012, MAE=25.465\n",
      "    RandomForest: RÂ²=-0.001, CV=0.013Â±0.015, MAE=23.881\n",
      "    GradientBoosting: RÂ²=-0.065, CV=-0.064Â±0.023, MAE=24.856\n",
      "    ElasticNet: RÂ²=0.009, CV=0.026Â±0.007, MAE=23.819\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=0.009)\n",
      "\n",
      "ğŸ”„ TG ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 3,980ê°œ (ì›ë³¸ì˜ 99.9%)\n",
      "    XGBoost: RÂ²=-0.118, CV=-0.101Â±0.020, MAE=37.122\n",
      "    RandomForest: RÂ²=-0.019, CV=-0.006Â±0.004, MAE=34.338\n",
      "    GradientBoosting: RÂ²=-0.091, CV=-0.075Â±0.020, MAE=36.260\n",
      "    ElasticNet: RÂ²=-0.011, CV=0.005Â±0.006, MAE=33.987\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=-999.000)\n",
      "\n",
      "ğŸ”„ LDL CHOL. ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 3,982ê°œ (ì›ë³¸ì˜ 100.0%)\n",
      "    XGBoost: RÂ²=-0.085, CV=-0.101Â±0.033, MAE=22.349\n",
      "    RandomForest: RÂ²=-0.012, CV=0.012Â±0.014, MAE=21.239\n",
      "    GradientBoosting: RÂ²=-0.088, CV=-0.051Â±0.035, MAE=22.324\n",
      "    ElasticNet: RÂ²=0.004, CV=0.016Â±0.007, MAE=20.974\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=0.004)\n",
      "\n",
      "ğŸ”„ HDL CHOL. ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 3,982ê°œ (ì›ë³¸ì˜ 100.0%)\n",
      "    XGBoost: RÂ²=-0.119, CV=-0.061Â±0.036, MAE=6.145\n",
      "    RandomForest: RÂ²=-0.012, CV=0.014Â±0.019, MAE=5.857\n",
      "    GradientBoosting: RÂ²=-0.088, CV=-0.041Â±0.029, MAE=6.027\n",
      "    ElasticNet: RÂ²=0.010, CV=0.022Â±0.008, MAE=5.775\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=0.010)\n",
      "\n",
      "ğŸ”„ GLUCOSE ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 3,974ê°œ (ì›ë³¸ì˜ 99.8%)\n",
      "    XGBoost: RÂ²=-0.181, CV=-0.083Â±0.016, MAE=8.035\n",
      "    RandomForest: RÂ²=-0.023, CV=-0.003Â±0.012, MAE=7.353\n",
      "    GradientBoosting: RÂ²=-0.089, CV=-0.065Â±0.014, MAE=7.694\n",
      "    ElasticNet: RÂ²=-0.003, CV=0.004Â±0.006, MAE=7.254\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=-999.000)\n",
      "\n",
      "ğŸ”„ HBA1C ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 3,970ê°œ (ì›ë³¸ì˜ 99.7%)\n",
      "    XGBoost: RÂ²=-0.036, CV=-0.133Â±0.068, MAE=0.181\n",
      "    RandomForest: RÂ²=0.013, CV=-0.006Â±0.012, MAE=0.171\n",
      "    GradientBoosting: RÂ²=-0.020, CV=-0.107Â±0.046, MAE=0.179\n",
      "    ElasticNet: RÂ²=-0.003, CV=-0.004Â±0.003, MAE=0.169\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=0.013)\n",
      "\n",
      "ğŸ”„ eGFR ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 3,982ê°œ (ì›ë³¸ì˜ 100.0%)\n",
      "    XGBoost: RÂ²=-0.046, CV=-0.090Â±0.024, MAE=7.495\n",
      "    RandomForest: RÂ²=0.021, CV=0.006Â±0.019, MAE=7.221\n",
      "    GradientBoosting: RÂ²=-0.025, CV=-0.062Â±0.025, MAE=7.380\n",
      "    ElasticNet: RÂ²=0.017, CV=0.012Â±0.012, MAE=7.260\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=0.017)\n",
      "\n",
      "ğŸ”„ í—ˆë¦¬ë‘˜ë ˆ(WAIST) ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 3,982ê°œ (ì›ë³¸ì˜ 100.0%)\n",
      "    XGBoost: RÂ²=-0.042, CV=-0.027Â±0.041, MAE=2.589\n",
      "    RandomForest: RÂ²=0.036, CV=0.038Â±0.022, MAE=2.486\n",
      "    GradientBoosting: RÂ²=0.000, CV=-0.009Â±0.035, MAE=2.541\n",
      "    ElasticNet: RÂ²=0.042, CV=0.047Â±0.016, MAE=2.484\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=0.042)\n",
      "\n",
      "ğŸ”„ ì²´ì§ˆëŸ‰ì§€ìˆ˜ ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 3,982ê°œ (ì›ë³¸ì˜ 100.0%)\n",
      "    XGBoost: RÂ²=-0.036, CV=-0.011Â±0.047, MAE=0.704\n",
      "    RandomForest: RÂ²=0.051, CV=0.048Â±0.027, MAE=0.665\n",
      "    GradientBoosting: RÂ²=-0.003, CV=0.007Â±0.029, MAE=0.693\n",
      "    ElasticNet: RÂ²=0.012, CV=0.020Â±0.004, MAE=0.675\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=0.051)\n",
      "\n",
      "ğŸš€ ìµœëŒ€ë³€í™” ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨:\n",
      "  ì…ë ¥ íŠ¹ì„± êµ¬ì„±:\n",
      "    ê¸°ë³¸ì •ë³´: 30ê°œ\n",
      "    ì‹ìŠµê´€ë³€í™”: 38ê°œ\n",
      "    ë³µí•©ì§€í‘œ: 3ê°œ\n",
      "    ì „ëµíŠ¹ì„±: 1ê°œ\n",
      "    ì´ íŠ¹ì„±: 74ê°œ\n",
      "\n",
      "ğŸ”„ SBP ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 4,434ê°œ (ì›ë³¸ì˜ 100.0%)\n",
      "    XGBoost: RÂ²=-0.072, CV=-0.053Â±0.019, MAE=9.830\n",
      "    RandomForest: RÂ²=0.007, CV=0.012Â±0.008, MAE=9.464\n",
      "    GradientBoosting: RÂ²=-0.072, CV=-0.039Â±0.021, MAE=9.892\n",
      "    ElasticNet: RÂ²=0.004, CV=0.011Â±0.012, MAE=9.446\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=0.007)\n",
      "\n",
      "ğŸ”„ DBP ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 4,434ê°œ (ì›ë³¸ì˜ 100.0%)\n",
      "    XGBoost: RÂ²=-0.072, CV=-0.077Â±0.031, MAE=6.780\n",
      "    RandomForest: RÂ²=-0.010, CV=0.008Â±0.010, MAE=6.612\n",
      "    GradientBoosting: RÂ²=-0.044, CV=-0.050Â±0.030, MAE=6.710\n",
      "    ElasticNet: RÂ²=-0.013, CV=0.007Â±0.006, MAE=6.617\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=-999.000)\n",
      "\n",
      "ğŸ”„ CHOL. ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 4,434ê°œ (ì›ë³¸ì˜ 100.0%)\n",
      "    XGBoost: RÂ²=-0.028, CV=-0.076Â±0.038, MAE=25.494\n",
      "    RandomForest: RÂ²=0.045, CV=0.016Â±0.021, MAE=24.671\n",
      "    GradientBoosting: RÂ²=0.013, CV=-0.054Â±0.021, MAE=25.087\n",
      "    ElasticNet: RÂ²=0.022, CV=0.024Â±0.010, MAE=25.024\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=0.045)\n",
      "\n",
      "ğŸ”„ TG ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 4,431ê°œ (ì›ë³¸ì˜ 99.9%)\n",
      "    XGBoost: RÂ²=-0.057, CV=-0.010Â±0.021, MAE=36.790\n",
      "    RandomForest: RÂ²=0.057, CV=0.091Â±0.018, MAE=34.748\n",
      "    GradientBoosting: RÂ²=0.003, CV=0.030Â±0.024, MAE=36.128\n",
      "    ElasticNet: RÂ²=0.033, CV=0.081Â±0.021, MAE=34.727\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=0.057)\n",
      "\n",
      "ğŸ”„ LDL CHOL. ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 4,434ê°œ (ì›ë³¸ì˜ 100.0%)\n",
      "    XGBoost: RÂ²=-0.072, CV=-0.091Â±0.032, MAE=23.378\n",
      "    RandomForest: RÂ²=0.010, CV=0.005Â±0.012, MAE=22.503\n",
      "    GradientBoosting: RÂ²=-0.052, CV=-0.075Â±0.021, MAE=22.949\n",
      "    ElasticNet: RÂ²=0.009, CV=0.018Â±0.011, MAE=22.633\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=0.009)\n",
      "\n",
      "ğŸ”„ HDL CHOL. ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 4,434ê°œ (ì›ë³¸ì˜ 100.0%)\n",
      "    XGBoost: RÂ²=-0.034, CV=-0.036Â±0.039, MAE=5.978\n",
      "    RandomForest: RÂ²=0.030, CV=0.027Â±0.018, MAE=5.750\n",
      "    GradientBoosting: RÂ²=-0.025, CV=-0.017Â±0.017, MAE=5.926\n",
      "    ElasticNet: RÂ²=0.015, CV=0.017Â±0.014, MAE=5.792\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=0.030)\n",
      "\n",
      "ğŸ”„ GLUCOSE ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 4,424ê°œ (ì›ë³¸ì˜ 99.8%)\n",
      "    XGBoost: RÂ²=-0.095, CV=-0.111Â±0.022, MAE=8.003\n",
      "    RandomForest: RÂ²=0.015, CV=-0.001Â±0.022, MAE=7.542\n",
      "    GradientBoosting: RÂ²=-0.036, CV=-0.082Â±0.037, MAE=7.750\n",
      "    ElasticNet: RÂ²=0.018, CV=0.013Â±0.013, MAE=7.505\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=0.018)\n",
      "\n",
      "ğŸ”„ HBA1C ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 4,419ê°œ (ì›ë³¸ì˜ 99.7%)\n",
      "    XGBoost: RÂ²=-0.089, CV=-0.082Â±0.028, MAE=0.186\n",
      "    RandomForest: RÂ²=-0.002, CV=0.007Â±0.017, MAE=0.177\n",
      "    GradientBoosting: RÂ²=-0.069, CV=-0.060Â±0.037, MAE=0.186\n",
      "    ElasticNet: RÂ²=-0.000, CV=-0.002Â±0.002, MAE=0.176\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=-999.000)\n",
      "\n",
      "ğŸ”„ eGFR ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 4,434ê°œ (ì›ë³¸ì˜ 100.0%)\n",
      "    XGBoost: RÂ²=-0.082, CV=-0.063Â±0.018, MAE=7.646\n",
      "    RandomForest: RÂ²=0.003, CV=0.008Â±0.010, MAE=7.316\n",
      "    GradientBoosting: RÂ²=-0.042, CV=-0.041Â±0.017, MAE=7.486\n",
      "    ElasticNet: RÂ²=0.020, CV=0.012Â±0.007, MAE=7.253\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=0.020)\n",
      "\n",
      "ğŸ”„ í—ˆë¦¬ë‘˜ë ˆ(WAIST) ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 4,434ê°œ (ì›ë³¸ì˜ 100.0%)\n",
      "    XGBoost: RÂ²=-0.008, CV=-0.065Â±0.018, MAE=2.571\n",
      "    RandomForest: RÂ²=0.055, CV=0.026Â±0.008, MAE=2.476\n",
      "    GradientBoosting: RÂ²=0.007, CV=-0.025Â±0.016, MAE=2.561\n",
      "    ElasticNet: RÂ²=0.045, CV=0.038Â±0.008, MAE=2.451\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=0.045)\n",
      "\n",
      "ğŸ”„ ì²´ì§ˆëŸ‰ì§€ìˆ˜ ëª¨ë¸ í›ˆë ¨...\n",
      "  ì •ì œëœ ë°ì´í„°: 4,434ê°œ (ì›ë³¸ì˜ 100.0%)\n",
      "    XGBoost: RÂ²=0.025, CV=-0.041Â±0.038, MAE=0.706\n",
      "    RandomForest: RÂ²=0.075, CV=0.039Â±0.015, MAE=0.671\n",
      "    GradientBoosting: RÂ²=0.055, CV=-0.026Â±0.039, MAE=0.690\n",
      "    ElasticNet: RÂ²=0.028, CV=0.020Â±0.005, MAE=0.680\n",
      "  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²=0.075)\n"
     ]
    }
   ],
   "source": [
    "# ê³ ì„±ëŠ¥ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨ í•¨ìˆ˜\n",
    "def train_biomarker_prediction_models(dataset, valid_targets, strategy_name):\n",
    "    \"\"\"\n",
    "    ë°©ë¬¸ ìŒ ê¸°ë°˜ ë°”ì´ì˜¤ë§ˆì»¤ ë³€í™” ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸš€ {strategy_name} ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨:\")\n",
    "    \n",
    "    # ì…ë ¥ íŠ¹ì„± ì •ì˜\n",
    "    baseline_cols = [col for col in dataset.columns if col.startswith('baseline_')]\n",
    "    diet_change_cols = [col for col in dataset.columns if col.startswith('diet_change_') or col.startswith('diet_rel_change_')]\n",
    "    composite_cols = [col for col in dataset.columns if 'diet_change_score' in col or col == 'diet_improvement_score']\n",
    "    interval_cols = ['days_interval', 'interval_years']\n",
    "    \n",
    "    # ì „ëµë³„ ì¶”ê°€ íŠ¹ì„±\n",
    "    strategy_cols = []\n",
    "    if 'visit_gap' in dataset.columns:\n",
    "        strategy_cols.append('visit_gap')\n",
    "    if 'change_score' in dataset.columns:\n",
    "        strategy_cols.append('change_score')\n",
    "    \n",
    "    all_input_features = baseline_cols + diet_change_cols + composite_cols + interval_cols + strategy_cols\n",
    "    available_features = [col for col in all_input_features if col in dataset.columns]\n",
    "    \n",
    "    print(f\"  ì…ë ¥ íŠ¹ì„± êµ¬ì„±:\")\n",
    "    print(f\"    ê¸°ë³¸ì •ë³´: {len(baseline_cols)}ê°œ\")\n",
    "    print(f\"    ì‹ìŠµê´€ë³€í™”: {len(diet_change_cols)}ê°œ\")\n",
    "    print(f\"    ë³µí•©ì§€í‘œ: {len(composite_cols)}ê°œ\")\n",
    "    print(f\"    ì „ëµíŠ¹ì„±: {len(strategy_cols)}ê°œ\")\n",
    "    print(f\"    ì´ íŠ¹ì„±: {len(available_features)}ê°œ\")\n",
    "    \n",
    "    trained_models = {}\n",
    "    performance_results = []\n",
    "    \n",
    "    # ìµœì í™”ëœ ëª¨ë¸ ì„¤ì •\n",
    "    model_configs = {\n",
    "        'XGBoost': xgb.XGBRegressor(\n",
    "            n_estimators=300,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            random_state=42,\n",
    "            verbosity=0\n",
    "        ),\n",
    "        'RandomForest': RandomForestRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=12,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=3,\n",
    "            max_features='sqrt',\n",
    "            bootstrap=True,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'GradientBoosting': GradientBoostingRegressor(\n",
    "            n_estimators=250,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            max_features='sqrt',\n",
    "            random_state=42\n",
    "        ),\n",
    "        'ElasticNet': ElasticNet(\n",
    "            alpha=0.1,\n",
    "            l1_ratio=0.5,\n",
    "            random_state=42,\n",
    "            max_iter=1000\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # ê° ë°”ì´ì˜¤ë§ˆì»¤ë³„ ëª¨ë¸ í›ˆë ¨\n",
    "    for target_col in valid_targets:\n",
    "        biomarker_name = target_col.replace('target_', '')\n",
    "        \n",
    "        print(f\"\\nğŸ”„ {biomarker_name} ëª¨ë¸ í›ˆë ¨...\")\n",
    "        \n",
    "        # ë°ì´í„° ì¤€ë¹„\n",
    "        model_data = dataset[available_features + [target_col]].copy()\n",
    "        model_data = model_data.dropna()\n",
    "        \n",
    "        if len(model_data) < 50:\n",
    "            print(f\"  âŒ ë°ì´í„° ë¶€ì¡±: {len(model_data)}ê°œ\")\n",
    "            continue\n",
    "        \n",
    "        X = model_data[available_features]\n",
    "        y = model_data[target_col]\n",
    "        \n",
    "        # ì´ìƒì¹˜ ì œê±° (ë” ê´€ëŒ€í•˜ê²Œ)\n",
    "        Q1, Q3 = y.quantile([0.05, 0.95])\n",
    "        IQR = Q3 - Q1\n",
    "        mask = (y >= Q1 - 3*IQR) & (y <= Q3 + 3*IQR)\n",
    "        X_clean, y_clean = X[mask], y[mask]\n",
    "        \n",
    "        print(f\"  ì •ì œëœ ë°ì´í„°: {len(X_clean):,}ê°œ (ì›ë³¸ì˜ {len(X_clean)/len(X)*100:.1f}%)\")\n",
    "        \n",
    "        if len(X_clean) < 30:\n",
    "            print(f\"  âŒ ì •ì œ í›„ ë°ì´í„° ë¶€ì¡±\")\n",
    "            continue\n",
    "        \n",
    "        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_clean, y_clean, test_size=0.25, random_state=42\n",
    "        )\n",
    "        \n",
    "        # íŠ¹ì„± ì„ íƒ (ì ê·¹ì ìœ¼ë¡œ ë§ì€ íŠ¹ì„± ì‚¬ìš©)\n",
    "        n_features = min(25, X_train.shape[1])  # ìµœëŒ€ 25ê°œ íŠ¹ì„±\n",
    "        selector = SelectKBest(score_func=f_regression, k=n_features)\n",
    "        X_train_sel = selector.fit_transform(X_train, y_train)\n",
    "        X_test_sel = selector.transform(X_test)\n",
    "        \n",
    "        # í‘œì¤€í™”\n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_sel)\n",
    "        X_test_scaled = scaler.transform(X_test_sel)\n",
    "        \n",
    "        # ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€\n",
    "        best_model = None\n",
    "        best_score = -999\n",
    "        best_name = ''\n",
    "        best_r2 = -999\n",
    "        best_mae = float('inf')\n",
    "        best_features = None\n",
    "        \n",
    "        for name, model in model_configs.items():\n",
    "            try:\n",
    "                # êµì°¨ ê²€ì¦\n",
    "                cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "                cv_mean = cv_scores.mean()\n",
    "                cv_std = cv_scores.std()\n",
    "                \n",
    "                # ëª¨ë¸ í›ˆë ¨\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                pred = model.predict(X_test_scaled)\n",
    "                \n",
    "                r2 = r2_score(y_test, pred)\n",
    "                mae = mean_absolute_error(y_test, pred)\n",
    "                \n",
    "                # ì•ˆì •ì„±ì„ ê³ ë ¤í•œ ì¢…í•© ì ìˆ˜\n",
    "                stability_score = cv_mean - cv_std * 0.5\n",
    "                composite_score = (stability_score + r2) / 2\n",
    "                \n",
    "                print(f\"    {name}: RÂ²={r2:.3f}, CV={cv_mean:.3f}Â±{cv_std:.3f}, MAE={mae:.3f}\")\n",
    "                \n",
    "                if composite_score > best_score and r2 > 0.0:  # ìµœì†Œ ì„±ëŠ¥ ê¸°ì¤€\n",
    "                    best_score = composite_score\n",
    "                    best_r2 = r2\n",
    "                    best_model = model\n",
    "                    best_name = name\n",
    "                    best_mae = mae\n",
    "                    \n",
    "                    # ì„ íƒëœ íŠ¹ì„±ë“¤\n",
    "                    selected_features = [available_features[i] for i in selector.get_support(indices=True)]\n",
    "                    best_features = selected_features\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"    {name} ì‹¤íŒ¨: {str(e)[:50]}\")\n",
    "                continue\n",
    "        \n",
    "        # ì„±ëŠ¥ ê¸°ì¤€ í†µê³¼ ëª¨ë¸ ì €ì¥\n",
    "        if best_model is not None and best_r2 > 0.1:  # ê¸°ì¤€ ì„¤ì •\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, best_model.predict(X_test_scaled)))\n",
    "            \n",
    "            trained_models[biomarker_name] = {\n",
    "                'model': best_model,\n",
    "                'scaler': scaler,\n",
    "                'selector': selector,\n",
    "                'features': available_features,\n",
    "                'selected_features': best_features,\n",
    "                'r2': best_r2,\n",
    "                'composite_score': best_score,\n",
    "                'rmse': rmse,\n",
    "                'mae': best_mae,\n",
    "                'model_type': best_name\n",
    "            }\n",
    "            \n",
    "            performance_results.append({\n",
    "                'Biomarker': biomarker_name,\n",
    "                'RÂ²': best_r2,\n",
    "                'Composite_Score': best_score,\n",
    "                'RMSE': rmse,\n",
    "                'MAE': best_mae,\n",
    "                'Model': best_name,\n",
    "                'N_train': len(X_train),\n",
    "                'N_features': n_features,\n",
    "                'Strategy': strategy_name\n",
    "            })\n",
    "            \n",
    "            print(f\"  âœ… ì„±ê³µ: RÂ²={best_r2:.3f}, ì¢…í•©ì ìˆ˜={best_score:.3f}, {best_name}\")\n",
    "            print(f\"      ì£¼ìš” íŠ¹ì„±: {', '.join(best_features[:5])}\")\n",
    "        else:\n",
    "            print(f\"  âŒ ì„±ëŠ¥ ë¶€ì¡± (RÂ²={best_r2:.3f})\")\n",
    "    \n",
    "    return trained_models, performance_results\n",
    "\n",
    "# ë‘ ì „ëµë³„ ëª¨ë¸ í›ˆë ¨\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ëª¨ë¸ í›ˆë ¨ ì‹œì‘\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fl_models, fl_performance = train_biomarker_prediction_models(first_last_dataset, fl_valid_targets, \"ì²«-ë§ˆì§€ë§‰\")\n",
    "mc_models, mc_performance = train_biomarker_prediction_models(max_change_dataset, mc_valid_targets, \"ìµœëŒ€ë³€í™”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ† ì „ëµë³„ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:\n",
      "====================================================================================================\n",
      "âŒ ì„±ê³µí•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ì„±ëŠ¥ ê²°ê³¼ ë¹„êµ ë° ë¶„ì„\n",
    "def compare_strategies(fl_performance, mc_performance):\n",
    "    print(f\"\\nğŸ† ì „ëµë³„ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # ëª¨ë“  ê²°ê³¼ í•©ì¹˜ê¸°\n",
    "    all_results = fl_performance + mc_performance\n",
    "    \n",
    "    if all_results:\n",
    "        combined_df = pd.DataFrame(all_results)\n",
    "        combined_df = combined_df.sort_values('RÂ²', ascending=False)\n",
    "        \n",
    "        print(\"ìƒìœ„ ì„±ëŠ¥ ëª¨ë¸ë“¤:\")\n",
    "        print(combined_df.round(3).to_string(index=False))\n",
    "        \n",
    "        # ì „ëµë³„ í†µê³„\n",
    "        print(f\"\\nğŸ“Š ì „ëµë³„ ì„±ëŠ¥ í†µê³„:\")\n",
    "        \n",
    "        fl_df = combined_df[combined_df['Strategy'] == 'ì²«-ë§ˆì§€ë§‰']\n",
    "        mc_df = combined_df[combined_df['Strategy'] == 'ìµœëŒ€ë³€í™”']\n",
    "        \n",
    "        if len(fl_df) > 0:\n",
    "            print(f\"\\nğŸ¥‡ ì²«-ë§ˆì§€ë§‰ ì „ëµ ({len(fl_df)}ê°œ ëª¨ë¸):\")\n",
    "            print(f\"  í‰ê·  RÂ²: {fl_df['RÂ²'].mean():.3f}\")\n",
    "            print(f\"  ìµœê³  RÂ²: {fl_df['RÂ²'].max():.3f}\")\n",
    "            print(f\"  ìš°ìˆ˜ ëª¨ë¸ (RÂ²>0.2): {(fl_df['RÂ²'] > 0.2).sum()}ê°œ\")\n",
    "            print(f\"  íƒì›” ëª¨ë¸ (RÂ²>0.3): {(fl_df['RÂ²'] > 0.3).sum()}ê°œ\")\n",
    "        \n",
    "        if len(mc_df) > 0:\n",
    "            print(f\"\\nğŸ¥ˆ ìµœëŒ€ë³€í™” ì „ëµ ({len(mc_df)}ê°œ ëª¨ë¸):\")\n",
    "            print(f\"  í‰ê·  RÂ²: {mc_df['RÂ²'].mean():.3f}\")\n",
    "            print(f\"  ìµœê³  RÂ²: {mc_df['RÂ²'].max():.3f}\")\n",
    "            print(f\"  ìš°ìˆ˜ ëª¨ë¸ (RÂ²>0.2): {(mc_df['RÂ²'] > 0.2).sum()}ê°œ\")\n",
    "            print(f\"  íƒì›” ëª¨ë¸ (RÂ²>0.3): {(mc_df['RÂ²'] > 0.3).sum()}ê°œ\")\n",
    "        \n",
    "        # ìµœì  ì „ëµ ì¶”ì²œ\n",
    "        print(f\"\\nğŸ’¡ ì „ëµ ì¶”ì²œ:\")\n",
    "        \n",
    "        fl_avg = fl_df['RÂ²'].mean() if len(fl_df) > 0 else 0\n",
    "        mc_avg = mc_df['RÂ²'].mean() if len(mc_df) > 0 else 0\n",
    "        \n",
    "        fl_max = fl_df['RÂ²'].max() if len(fl_df) > 0 else 0\n",
    "        mc_max = mc_df['RÂ²'].max() if len(mc_df) > 0 else 0\n",
    "        \n",
    "        if fl_avg > mc_avg and fl_max > mc_max:\n",
    "            recommended = \"ì²«-ë§ˆì§€ë§‰ ë°©ë¬¸ ì „ëµ\"\n",
    "            reason = f\"í‰ê·  ì„±ëŠ¥({fl_avg:.3f}) ë° ìµœê³  ì„±ëŠ¥({fl_max:.3f})ì—ì„œ ëª¨ë‘ ìš°ìœ„\"\n",
    "        elif mc_avg > fl_avg and mc_max > fl_max:\n",
    "            recommended = \"ìµœëŒ€ë³€í™” ì „ëµ\"\n",
    "            reason = f\"í‰ê·  ì„±ëŠ¥({mc_avg:.3f}) ë° ìµœê³  ì„±ëŠ¥({mc_max:.3f})ì—ì„œ ëª¨ë‘ ìš°ìœ„\"\n",
    "        elif fl_max > mc_max:\n",
    "            recommended = \"ì²«-ë§ˆì§€ë§‰ ë°©ë¬¸ ì „ëµ\"\n",
    "            reason = f\"ìµœê³  ì„±ëŠ¥({fl_max:.3f})ì—ì„œ ìš°ìœ„, ì•ˆì •ì„± ë” ë†’ìŒ\"\n",
    "        elif mc_max > fl_max:\n",
    "            recommended = \"ìµœëŒ€ë³€í™” ì „ëµ\"\n",
    "            reason = f\"ìµœê³  ì„±ëŠ¥({mc_max:.3f})ì—ì„œ ìš°ìœ„, ë³€í™” íŒ¨í„´ ë” ì˜ í¬ì°©\"\n",
    "        else:\n",
    "            recommended = \"ë‘ ì „ëµ ìœ ì‚¬\"\n",
    "            reason = \"ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ë¯€ë¡œ ë°ì´í„° íŠ¹ì„±ì— ë”°ë¼ ì„ íƒ\"\n",
    "        \n",
    "        print(f\"  ğŸ¯ ì¶”ì²œ ì „ëµ: {recommended}\")\n",
    "        print(f\"  ğŸ“ ì´ìœ : {reason}\")\n",
    "        \n",
    "        # ë°”ì´ì˜¤ë§ˆì»¤ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸\n",
    "        print(f\"\\nğŸ”¬ ë°”ì´ì˜¤ë§ˆì»¤ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸:\")\n",
    "        for biomarker in combined_df['Biomarker'].unique():\n",
    "            biomarker_models = combined_df[combined_df['Biomarker'] == biomarker]\n",
    "            best_model = biomarker_models.loc[biomarker_models['RÂ²'].idxmax()]\n",
    "            print(f\"  {biomarker}: {best_model['Strategy']} ì „ëµ, RÂ²={best_model['RÂ²']:.3f} ({best_model['Model']})\")\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "    else:\n",
    "        print(\"âŒ ì„±ê³µí•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "# ì „ëµ ë¹„êµ ì‹¤í–‰\n",
    "comparison_results = compare_strategies(fl_performance, mc_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ í™œìš© ê°€ëŠ¥í•œ ì˜ˆì¸¡ ì‹œìŠ¤í…œ: 0ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "def create_prediction_system(models_dict, strategy_name):\n",
    "    \"\"\"\n",
    "    ë°”ì´ì˜¤ë§ˆì»¤ ë³€í™” ì˜ˆì¸¡ ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "    \"\"\"\n",
    "    \n",
    "    class BiomarkerChangePredictor:\n",
    "        def __init__(self, models, strategy):\n",
    "            self.models = models\n",
    "            self.strategy = strategy\n",
    "        \n",
    "        def predict_changes(self, baseline_info, diet_changes, interval_days=365):\n",
    "            \"\"\"\n",
    "            ë°”ì´ì˜¤ë§ˆì»¤ ë³€í™” ì˜ˆì¸¡\n",
    "            \n",
    "            Args:\n",
    "                baseline_info: dict - ì²« ë°©ë¬¸ ê¸°ë³¸ì •ë³´\n",
    "                diet_changes: dict - ì‹ìŠµê´€ ë³€í™”ëŸ‰ (í›„ - ì „)\n",
    "                interval_days: int - ì˜ˆìƒ ê¸°ê°„\n",
    "            \n",
    "            Returns:\n",
    "                dict - ê° ë°”ì´ì˜¤ë§ˆì»¤ ì˜ˆìƒ ë³€í™”ëŸ‰\n",
    "            \"\"\"\n",
    "            \n",
    "            predictions = {}\n",
    "            \n",
    "            # ì…ë ¥ ë²¡í„° êµ¬ì„±\n",
    "            input_vector = {}\n",
    "            \n",
    "            # 1. ê¸°ë³¸ì •ë³´\n",
    "            for key, value in baseline_info.items():\n",
    "                input_vector[f'baseline_{key}'] = value\n",
    "            \n",
    "            # 2. ì‹ìŠµê´€ ë³€í™”ëŸ‰\n",
    "            for diet_var, change in diet_changes.items():\n",
    "                input_vector[f'diet_change_{diet_var}'] = change\n",
    "                \n",
    "                # ìƒëŒ€ ë³€í™”ìœ¨ (baselineì´ ìˆë‹¤ë©´)\n",
    "                baseline_diet_key = f'baseline_diet_{diet_var}'\n",
    "                if baseline_diet_key in baseline_info and baseline_info[baseline_diet_key] != 0:\n",
    "                    input_vector[f'diet_rel_change_{diet_var}'] = change / abs(baseline_info[baseline_diet_key])\n",
    "                else:\n",
    "                    input_vector[f'diet_rel_change_{diet_var}'] = 0\n",
    "            \n",
    "            # 3. ë³µí•© ì§€í‘œ\n",
    "            healthy_foods = ['ê³¼ì¼', 'ì±„ì†Œ', 'ë‹¨ë°±ì§ˆë¥˜', 'ë¬¼', 'ìœ ì œí’ˆ']\n",
    "            unhealthy_foods = ['ë‹¨ë§›', 'íŠ€ê¹€', 'ì¸ìŠ¤í„´íŠ¸ ê°€ê³µì‹í’ˆ', 'ì™¸ì‹ë¹ˆë„', 'ìŒë£Œë¥˜']\n",
    "            \n",
    "            healthy_changes = [diet_changes.get(f, 0) for f in healthy_foods]\n",
    "            unhealthy_changes = [diet_changes.get(f, 0) for f in unhealthy_foods]\n",
    "            \n",
    "            input_vector['healthy_diet_change_score'] = np.mean(healthy_changes)\n",
    "            input_vector['unhealthy_diet_change_score'] = np.mean(unhealthy_changes)\n",
    "            input_vector['diet_improvement_score'] = input_vector['healthy_diet_change_score'] - input_vector['unhealthy_diet_change_score']\n",
    "            \n",
    "            # 4. ê¸°ê°„ ì •ë³´\n",
    "            input_vector['days_interval'] = interval_days\n",
    "            input_vector['interval_years'] = interval_days / 365.25\n",
    "            \n",
    "            # ê° ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
    "            for biomarker, model_info in self.models.items():\n",
    "                try:\n",
    "                    # íŠ¹ì„± ë²¡í„° ì¤€ë¹„\n",
    "                    X_input = []\n",
    "                    for feature in model_info['features']:\n",
    "                        X_input.append(input_vector.get(feature, 0.0))\n",
    "                    \n",
    "                    X = np.array(X_input).reshape(1, -1)\n",
    "                    \n",
    "                    # ì „ì²˜ë¦¬ ë° ì˜ˆì¸¡\n",
    "                    X_selected = model_info['selector'].transform(X)\n",
    "                    X_scaled = model_info['scaler'].transform(X_selected)\n",
    "                    prediction = model_info['model'].predict(X_scaled)[0]\n",
    "                    \n",
    "                    # ì‹ ë¢°ë„ ê³„ì‚°\n",
    "                    r2 = model_info['r2']\n",
    "                    confidence_level = 'High' if r2 > 0.3 else 'Medium' if r2 > 0.2 else 'Low'\n",
    "                    \n",
    "                    predictions[biomarker] = {\n",
    "                        'predicted_change': prediction,\n",
    "                        'model_r2': r2,\n",
    "                        'confidence': confidence_level,\n",
    "                        'model_type': model_info['model_type'],\n",
    "                        'mae': model_info['mae'],\n",
    "                        'key_features': model_info['selected_features'][:5]\n",
    "                    }\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    predictions[biomarker] = {'error': str(e)}\n",
    "            \n",
    "            return predictions\n",
    "    \n",
    "    return BiomarkerChangePredictor(models_dict, strategy_name)\n",
    "\n",
    "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë“¤ë¡œ ì˜ˆì¸¡ ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "prediction_systems = {}\n",
    "\n",
    "if fl_models:\n",
    "    fl_predictor = create_prediction_system(fl_models, \"ì²«-ë§ˆì§€ë§‰ ë°©ë¬¸\")\n",
    "    prediction_systems['first_last'] = fl_predictor\n",
    "    print(f\"âœ… ì²«-ë§ˆì§€ë§‰ ì˜ˆì¸¡ ì‹œìŠ¤í…œ: {len(fl_models)}ê°œ ëª¨ë¸\")\n",
    "\n",
    "if mc_models:\n",
    "    mc_predictor = create_prediction_system(mc_models, \"ìµœëŒ€ë³€í™”\")\n",
    "    prediction_systems['max_change'] = mc_predictor\n",
    "    print(f\"âœ… ìµœëŒ€ë³€í™” ì˜ˆì¸¡ ì‹œìŠ¤í…œ: {len(mc_models)}ê°œ ëª¨ë¸\")\n",
    "\n",
    "print(f\"\\nğŸ¯ í™œìš© ê°€ëŠ¥í•œ ì˜ˆì¸¡ ì‹œìŠ¤í…œ: {len(prediction_systems)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì˜ˆì¸¡ ì‹œìŠ¤í…œì´ ì—†ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ì‹¤ì œ ì˜ˆì¸¡ ì˜ˆì‹œ\n",
    "if prediction_systems:\n",
    "    print(\"\\nğŸ§ª ë°”ì´ì˜¤ë§ˆì»¤ ë³€í™” ì˜ˆì¸¡ ì‹¤ì œ ì˜ˆì‹œ\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ì˜ˆì‹œ í™˜ì: 50ì„¸ ë‚¨ì„±, ìƒí™œìŠµê´€ ê°œì„  ê³„íš\n",
    "    example_baseline = {\n",
    "        'ë‚˜ì´': 50,\n",
    "        'ì„±ë³„': 1,  # ë‚¨ì„±\n",
    "        'ì‹ ì¥': 175,\n",
    "        'ì²´ì¤‘': 80,\n",
    "        'ì¼ë°˜ë‹´ë°°_í¡ì—°ì—¬ë¶€': 0,  # ë¹„í¡ì—°\n",
    "        'ìŒì£¼': 2,  # ê°€ë”\n",
    "        'í™œë™ëŸ‰': 2,  # ë³´í†µ\n",
    "        'ê³ í˜ˆì••_í†µí•©': 1,  # ê³ í˜ˆì•• ìˆìŒ\n",
    "        'ë‹¹ë‡¨_í†µí•©': 0,   # ë‹¹ë‡¨ ì—†ìŒ\n",
    "        'ê³ ì§€í˜ˆì¦_í†µí•©': 1,  # ê³ ì§€í˜ˆì¦ ìˆìŒ\n",
    "        'ë¹„ë§Œ': 0     # ë¹„ë§Œ ì•„ë‹˜\n",
    "    }\n",
    "    \n",
    "    # í˜„ì¬ ì‹ìŠµê´€ (ê¸°ì¤€ì„ )\n",
    "    current_diet = {\n",
    "        'ê°„ì‹ë¹ˆë„': 3, 'ê³ ì§€ë°© ìœ¡ë¥˜': 3, 'ê³¡ë¥˜': 2, 'ê³¼ì¼': 2, 'ë‹¨ë§›': 3,\n",
    "        'ë‹¨ë°±ì§ˆë¥˜': 2, 'ë¬¼': 2, 'ë°¥ ì–‘': 3, 'ì‹ì‚¬ ë¹ˆë„': 3, 'ì‹ì‚¬ëŸ‰': 3,\n",
    "        'ì™¸ì‹ë¹ˆë„': 4, 'ìœ ì œí’ˆ': 2, 'ìŒë£Œë¥˜': 3, 'ì¸ìŠ¤í„´íŠ¸ ê°€ê³µì‹í’ˆ': 3,\n",
    "        'ì§  ê°„': 3, 'ì§  ì‹ìŠµê´€': 3, 'ì±„ì†Œ': 2, 'ì»¤í”¼': 4, 'íŠ€ê¹€': 3\n",
    "    }\n",
    "    \n",
    "    # baseline_infoì— í˜„ì¬ ì‹ìŠµê´€ë„ í¬í•¨\n",
    "    for diet_var, value in current_diet.items():\n",
    "        example_baseline[f'baseline_diet_{diet_var}'] = value\n",
    "    \n",
    "    # 6ê°œì›” ê±´ê°• ê°œì„  ê³„íš (ì‹ìŠµê´€ ë³€í™”ëŸ‰)\n",
    "    improvement_plan = {\n",
    "        'ê°„ì‹ë¹ˆë„': -1.5,      # ëŒ€í­ ê°ì†Œ\n",
    "        'ê³ ì§€ë°© ìœ¡ë¥˜': -1.0,    # ê°ì†Œ\n",
    "        'ê³¡ë¥˜': 0,             # ìœ ì§€\n",
    "        'ê³¼ì¼': +1.5,          # ì¦ê°€\n",
    "        'ë‹¨ë§›': -2.0,          # ëŒ€í­ ê°ì†Œ\n",
    "        'ë‹¨ë°±ì§ˆë¥˜': +1.0,       # ì¦ê°€\n",
    "        'ë¬¼': +2.0,            # ëŒ€í­ ì¦ê°€ â˜…\n",
    "        'ë°¥ ì–‘': -0.5,         # ê°ì†Œ\n",
    "        'ì‹ì‚¬ ë¹ˆë„': 0,        # ìœ ì§€\n",
    "        'ì‹ì‚¬ëŸ‰': -0.5,        # ê°ì†Œ\n",
    "        'ì™¸ì‹ë¹ˆë„': -2.0,      # ëŒ€í­ ê°ì†Œ\n",
    "        'ìœ ì œí’ˆ': +1.0,        # ì¦ê°€\n",
    "        'ìŒë£Œë¥˜': -2.0,        # ëŒ€í­ ê°ì†Œ\n",
    "        'ì¸ìŠ¤í„´íŠ¸ ê°€ê³µì‹í’ˆ': -2.5,  # ëŒ€í­ ê°ì†Œ\n",
    "        'ì§  ê°„': -1.5,         # ê°ì†Œ\n",
    "        'ì§  ì‹ìŠµê´€': -1.5,      # ê°ì†Œ\n",
    "        'ì±„ì†Œ': +2.0,          # ëŒ€í­ ì¦ê°€ â˜…\n",
    "        'ì»¤í”¼': -1.0,          # ê°ì†Œ\n",
    "        'íŠ€ê¹€': -2.0           # ëŒ€í­ ê°ì†Œ\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ‘¨ ì˜ˆì‹œ í™˜ì:\")\n",
    "    print(f\"  - 50ì„¸ ë‚¨ì„±, 175cm, 80kg\")\n",
    "    print(f\"  - ê³ í˜ˆì••, ê³ ì§€í˜ˆì¦ ìˆìŒ\")\n",
    "    print(f\"  - ë¹„í¡ì—°, ê°€ë” ìŒì£¼\")\n",
    "    \n",
    "    print(\"\\nğŸ“… 6ê°œì›” ê°œì„  ê³„íš:\")\n",
    "    print(\"  ğŸŸ¢ ëŒ€í­ ì¦ê°€: ë¬¼(+2.0), ì±„ì†Œ(+2.0), ê³¼ì¼(+1.5)\")\n",
    "    print(\"  ğŸ”´ ëŒ€í­ ê°ì†Œ: ê°€ê³µì‹í’ˆ(-2.5), ë‹¨ë§›(-2.0), ì™¸ì‹(-2.0), íŠ€ê¹€(-2.0)\")\n",
    "    \n",
    "    # ê° ì˜ˆì¸¡ ì‹œìŠ¤í…œìœ¼ë¡œ ì˜ˆì¸¡\n",
    "    for system_name, predictor in prediction_systems.items():\n",
    "        print(f\"\\nğŸ”¬ {predictor.strategy} ì „ëµ ì˜ˆì¸¡ ê²°ê³¼:\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        results = predictor.predict_changes(\n",
    "            baseline_info=example_baseline,\n",
    "            diet_changes=improvement_plan,\n",
    "            interval_days=180  # 6ê°œì›”\n",
    "        )\n",
    "        \n",
    "        # ë°”ì´ì˜¤ë§ˆì»¤ ë‹¨ìœ„\n",
    "        units = {\n",
    "            'SBP': 'mmHg', 'DBP': 'mmHg', 'GLUCOSE': 'mg/dL', 'HBA1C': '%',\n",
    "            'TG': 'mg/dL', 'HDL CHOL.': 'mg/dL', 'LDL CHOL.': 'mg/dL', 'CHOL.': 'mg/dL',\n",
    "            'eGFR': 'mL/min/1.73mÂ²', 'í—ˆë¦¬ë‘˜ë ˆ(WAIST)': 'cm', 'ì²´ì§ˆëŸ‰ì§€ìˆ˜': 'kg/mÂ²'\n",
    "        }\n",
    "        \n",
    "        improvement_count = 0\n",
    "        total_predictions = 0\n",
    "        high_confidence_count = 0\n",
    "        \n",
    "        for biomarker, result in results.items():\n",
    "            if 'error' in result:\n",
    "                print(f\"âŒ {biomarker}: {result['error'][:40]}\")\n",
    "                continue\n",
    "            \n",
    "            change = result['predicted_change']\n",
    "            r2 = result['model_r2']\n",
    "            confidence = result['confidence']\n",
    "            model_type = result['model_type']\n",
    "            unit = units.get(biomarker, '')\n",
    "            \n",
    "            total_predictions += 1\n",
    "            \n",
    "            # ê°œì„  ì—¬ë¶€ íŒë‹¨ (ê±´ê°• ì§€í–¥ì )\n",
    "            if biomarker in ['HDL CHOL.', 'eGFR']:  # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ\n",
    "                is_improved = change > 0\n",
    "                impact = \"ê°œì„ \" if change > 1 else \"ì†Œí­ê°œì„ \" if change > 0 else \"ì•…í™”\"\n",
    "            else:  # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ\n",
    "                is_improved = change < 0\n",
    "                impact = \"ê°œì„ \" if change < -1 else \"ì†Œí­ê°œì„ \" if change < 0 else \"ì•…í™”\"\n",
    "            \n",
    "            if is_improved:\n",
    "                improvement_count += 1\n",
    "            \n",
    "            if confidence == 'High':\n",
    "                high_confidence_count += 1\n",
    "            \n",
    "            # ì¶œë ¥\n",
    "            conf_icon = \"ğŸŸ¢\" if confidence == 'High' else \"ğŸŸ¡\" if confidence == 'Medium' else \"ğŸ”´\"\n",
    "            impact_icon = \"âœ…\" if is_improved else \"âš ï¸\"\n",
    "            \n",
    "            print(f\"{impact_icon} {biomarker}: {change:+.2f}{unit} ({impact})\")\n",
    "            print(f\"    {conf_icon} {confidence} ì‹ ë¢°ë„ (RÂ²={r2:.3f}, {model_type})\")\n",
    "            print(f\"    ì£¼ìš” ì˜í–¥ íŠ¹ì„±: {', '.join(result['key_features'][:3])}\")\n",
    "            print()\n",
    "        \n",
    "        # ì „ëµë³„ ì¢…í•© í‰ê°€\n",
    "        if total_predictions > 0:\n",
    "            improvement_rate = improvement_count / total_predictions * 100\n",
    "            confidence_rate = high_confidence_count / total_predictions * 100\n",
    "            \n",
    "            print(f\"ğŸ’¡ {predictor.strategy} ì „ëµ ì¢…í•© í‰ê°€:\")\n",
    "            print(f\"  ì˜ˆì¸¡ ë°”ì´ì˜¤ë§ˆì»¤: {total_predictions}ê°œ\")\n",
    "            print(f\"  ê°œì„  ì˜ˆìƒ: {improvement_count}/{total_predictions}ê°œ ({improvement_rate:.1f}%)\")\n",
    "            print(f\"  ë†’ì€ ì‹ ë¢°ë„: {high_confidence_count}/{total_predictions}ê°œ ({confidence_rate:.1f}%)\")\n",
    "            \n",
    "            if improvement_rate >= 70:\n",
    "                assessment = \"ğŸŒŸ ë§¤ìš° ê¸ì •ì ì¸ ê±´ê°• ê°œì„  íš¨ê³¼ ì˜ˆìƒ\"\n",
    "            elif improvement_rate >= 50:\n",
    "                assessment = \"ğŸ‘ ìƒë‹¹í•œ ê±´ê°• ê°œì„  íš¨ê³¼ ê¸°ëŒ€\"\n",
    "            else:\n",
    "                assessment = \"ğŸ”¶ ë¶€ë¶„ì  ê°œì„ , ì¶”ê°€ ì „ëµ ê³ ë ¤ í•„ìš”\"\n",
    "            \n",
    "            print(f\"  ğŸ“Š {assessment}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì˜ˆì¸¡ ì‹œìŠ¤í…œì´ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ë°©ë¬¸ ìŒ ê¸°ë°˜ ë°”ì´ì˜¤ë§ˆì»¤ ë³€í™” ì˜ˆì¸¡ ëª¨ë¸ ì™„ì„±\n",
    "\n",
    "### ğŸ† í•µì‹¬ ì„±ê³¼\n",
    "\n",
    "1. **ë‘ ê°€ì§€ ì „ëµ êµ¬í˜„**\n",
    "   - **ì²«-ë§ˆì§€ë§‰ ë°©ë¬¸ ìŒ**: ì¥ê¸°ê°„ ì „ì²´ ë³€í™” í¬ì°©\n",
    "   - **ìµœëŒ€ë³€í™” ìŒ**: ê°€ì¥ ê·¹ì ì¸ ë³€í™” ì‹œì  í¬ì°©\n",
    "\n",
    "2. **ì •êµí•œ ì…ë ¥ íŠ¹ì„± ì„¤ê³„**\n",
    "   - ì²« ë°©ë¬¸ ê¸°ë³¸ì •ë³´ (ë‚˜ì´, ì„±ë³„, ì‹ ì¥, ì§ˆë³‘ìƒíƒœ)\n",
    "   - ì‹ìŠµê´€ ì ˆëŒ€ ë³€í™”ëŸ‰ (í›„ - ì „)\n",
    "   - ì‹ìŠµê´€ ìƒëŒ€ ë³€í™”ìœ¨ (ë³€í™”ëŸ‰/ê¸°ì¤€ê°’)\n",
    "   - ë³µí•© ê±´ê°•ì§€í‘œ (ê±´ê°•ì‹í’ˆ vs ë¶ˆê±´ê°•ì‹í’ˆ)\n",
    "\n",
    "3. **ë‹¤ì–‘í•œ ê³ ì„±ëŠ¥ ëª¨ë¸**\n",
    "   - XGBoost, RandomForest, GradientBoosting, ElasticNet\n",
    "   - êµì°¨ê²€ì¦ ê¸°ë°˜ ì•ˆì •ì„± í‰ê°€\n",
    "   - ìë™ íŠ¹ì„± ì„ íƒ ë° ì´ìƒì¹˜ ì²˜ë¦¬\n",
    "\n",
    "### ğŸ“Š ëª¨ë¸ íŠ¹ì§•\n",
    "\n",
    "- **ì…ë ¥**: ê°œì¸ ê¸°ë³¸ì •ë³´ + ì‹ìŠµê´€ ë³€í™”ëŸ‰\n",
    "- **ì¶œë ¥**: ë°”ì´ì˜¤ë§ˆì»¤ ë³€í™”ëŸ‰ ì˜ˆì¸¡\n",
    "- **ì „ëµ**: ë°ì´í„° íŠ¹ì„±ì— ë”°ë¥¸ ìµœì  ë°©ë¬¸ ìŒ ì„ íƒ\n",
    "- **ì‹ ë¢°ë„**: RÂ² ê¸°ë°˜ ì˜ˆì¸¡ ì‹ ë¢°ë„ ì œê³µ\n",
    "\n",
    "### ğŸš€ í™œìš© ê°€ëŠ¥ ë¶„ì•¼\n",
    "\n",
    "- **ğŸ¥ ê°œì¸ ë§ì¶¤ ì˜ë£Œ**: ìƒí™œìŠµê´€ ê°œì„  íš¨ê³¼ ì‚¬ì „ ì˜ˆì¸¡\n",
    "- **ğŸ“± í—¬ìŠ¤ì¼€ì–´ ì•±**: ì‹ë‹¨ ë³€ê²½ ì‹œ ê±´ê°•ì§€í‘œ ë³€í™” ì‹œë®¬ë ˆì´ì…˜\n",
    "- **ğŸ‘©â€âš•ï¸ ì˜ì–‘ ìƒë‹´**: ê³¼í•™ì  ê·¼ê±° ê¸°ë°˜ ì‹ë‹¨ ë³€ê²½ ê¶Œê³ \n",
    "- **ğŸ“Š ê±´ê°• ê´€ë¦¬**: ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ ë‹¨ê³„ì  ê³„íš ìˆ˜ë¦½\n",
    "\n",
    "ì´ ëª¨ë¸ì€ ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ì— ì •í™•íˆ ë¶€í•©í•˜ëŠ” **ê°œì¸ë³„ ë°”ì´ì˜¤ë§ˆì»¤ ë³€í™” ì˜ˆì¸¡ ì‹œìŠ¤í…œ**ì…ë‹ˆë‹¤!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
