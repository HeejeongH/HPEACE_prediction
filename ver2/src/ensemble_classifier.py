"""
Ver2: Ensemble Classification Model
====================================

ëª©ì : íšŒê·€ ëŒ€ì‹  ë¶„ë¥˜ ë¬¸ì œë¡œ ì ‘ê·¼í•˜ì—¬ ë†’ì€ ì •í™•ë„ ë‹¬ì„±
ë°©ë²•: Random Forest + XGBoost + LightGBM ì•™ìƒë¸”

íƒ€ê²Ÿ: 3-class ë¶„ë¥˜
- 0: ê°ì†Œ (decrease)
- 1: ìœ ì§€ (maintain)
- 2: ì¦ê°€ (increase)
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,
                            confusion_matrix, classification_report, roc_auc_score)
import xgboost as xgb
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os
from pathlib import Path

plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False


class EnsembleClassifier:
    """ì•™ìƒë¸” ë¶„ë¥˜ ëª¨ë¸"""
    
    def __init__(self, target_variable, random_state=42):
        """
        Args:
            target_variable: ì˜ˆì¸¡í•  ê±´ê°•ì§€í‘œ (ì˜ˆ: 'ì²´ì¤‘', 'BMI')
            random_state: ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ
        """
        self.target_variable = target_variable
        self.random_state = random_state
        self.model = None
        self.scaler_X = StandardScaler()
        self.feature_names = None
        self.metrics = {}
        self.class_names = ['ê°ì†Œ', 'ìœ ì§€', 'ì¦ê°€']
        
    def prepare_data(self, df):
        """ë°ì´í„° ì¤€ë¹„"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š [{self.target_variable}] ë¶„ë¥˜ ë°ì´í„° ì¤€ë¹„")
        print(f"{'='*80}")
        
        # 1. ì‹ìŠµê´€ ë³€í™” íŠ¹ì„±
        diet_change_cols = [col for col in df.columns 
                           if '_change' in col and 'ê±´ê°•' not in col 
                           and not any(bio in col for bio in ['ì²´ì¤‘', 'ì²´ì§ˆëŸ‰ì§€ìˆ˜', 'í—ˆë¦¬ë‘˜ë ˆ', 'SBP', 'DBP', 'TG'])]
        
        # 2. ë…ë¦½ì  ê±´ê°•ì§€í‘œ baseline
        obesity_indicators = ['ì²´ì¤‘', 'ì²´ì§ˆëŸ‰ì§€ìˆ˜', 'í—ˆë¦¬ë‘˜ë ˆ(WAIST)']
        bp_indicators = ['SBP', 'DBP']
        metabolic_indicators = ['TG']
        
        other_health_baselines = []
        
        if self.target_variable in obesity_indicators:
            for indicator in bp_indicators + metabolic_indicators:
                baseline_col = f'{indicator}_baseline'
                if baseline_col in df.columns:
                    other_health_baselines.append(baseline_col)
        elif self.target_variable in bp_indicators:
            for indicator in obesity_indicators + metabolic_indicators:
                baseline_col = f'{indicator}_baseline'
                if baseline_col in df.columns:
                    other_health_baselines.append(baseline_col)
        elif self.target_variable in metabolic_indicators:
            for indicator in obesity_indicators + bp_indicators:
                baseline_col = f'{indicator}_baseline'
                if baseline_col in df.columns:
                    other_health_baselines.append(baseline_col)
        
        # 3. íŒŒìƒ íŠ¹ì„±
        df_temp = df.copy()
        
        if 'ì²´ì§ˆëŸ‰ì§€ìˆ˜_baseline' in df_temp.columns:
            df_temp['BMI_category'] = pd.cut(
                df_temp['ì²´ì§ˆëŸ‰ì§€ìˆ˜_baseline'], 
                bins=[0, 18.5, 23, 25, 30, 100],
                labels=[0, 1, 2, 3, 4]
            ).astype(float)
        
        metabolic_risk_score = 0
        if 'ì²´ì§ˆëŸ‰ì§€ìˆ˜_baseline' in df_temp.columns:
            metabolic_risk_score += (df_temp['ì²´ì§ˆëŸ‰ì§€ìˆ˜_baseline'] >= 25).astype(int)
        if 'SBP_baseline' in df_temp.columns:
            metabolic_risk_score += (df_temp['SBP_baseline'] >= 130).astype(int)
        if 'DBP_baseline' in df_temp.columns:
            metabolic_risk_score += (df_temp['DBP_baseline'] >= 85).astype(int)
        if 'TG_baseline' in df_temp.columns:
            metabolic_risk_score += (df_temp['TG_baseline'] >= 150).astype(int)
        df_temp['metabolic_risk_score'] = metabolic_risk_score
        
        healthy_items = ['ì±„ì†Œ_change', 'ê³¼ì¼_change', 'ë‹¨ë°±ì§ˆë¥˜_change', 'ìœ ì œí’ˆ_change', 'ê³¡ë¥˜_change']
        healthy_score = sum(df_temp[item] for item in healthy_items if item in df_temp.columns)
        df_temp['healthy_eating_score'] = healthy_score
        
        unhealthy_items = ['ê°„ì‹ë¹ˆë„_change', 'ê³ ì§€ë°© ìœ¡ë¥˜_change', 'ë‹¨ë§›_change', 
                          'ìŒë£Œë¥˜_change', 'ì¸ìŠ¤í„´íŠ¸ ê°€ê³µì‹í’ˆ_change', 'ì§  ê°„_change', 
                          'ì§  ì‹ìŠµê´€_change', 'íŠ€ê¹€_change']
        unhealthy_score = sum(df_temp[item] for item in unhealthy_items if item in df_temp.columns)
        df_temp['unhealthy_eating_score'] = unhealthy_score
        
        df_temp['net_diet_improvement'] = df_temp['healthy_eating_score'] - df_temp['unhealthy_eating_score']
        
        # 4. ì „ì²´ íŠ¹ì„±
        additional_features = ['time_gap_days']
        derived_features = []
        
        for feat in ['BMI_category', 'metabolic_risk_score', 'healthy_eating_score', 
                     'unhealthy_eating_score', 'net_diet_improvement']:
            if feat in df_temp.columns:
                derived_features.append(feat)
        
        feature_cols = diet_change_cols + other_health_baselines + additional_features + derived_features
        self.feature_names = feature_cols
        
        # íƒ€ê²Ÿ: ë¶„ë¥˜ ë ˆì´ë¸”
        target_col = f'{self.target_variable}_class'
        
        if target_col not in df_temp.columns:
            raise ValueError(f"ë¶„ë¥˜ íƒ€ê²Ÿ '{target_col}'ì´ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # NaN ì œê±°
        valid_idx = df_temp[feature_cols + [target_col]].notna().all(axis=1)
        df_clean = df_temp[valid_idx].copy()
        
        X = df_clean[feature_cols].values
        y = df_clean[target_col].values
        
        print(f"\n   âœ… ìœ íš¨ ìƒ˜í”Œ: {len(df_clean):,}ê°œ")
        print(f"   âœ… íŠ¹ì„± ê°œìˆ˜: {len(feature_cols)}ê°œ")
        print(f"   âœ… íƒ€ê²Ÿ: {target_col} (3-class ë¶„ë¥˜)")
        
        # í´ë˜ìŠ¤ ë¶„í¬
        unique, counts = np.unique(y, return_counts=True)
        print(f"\n   ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬:")
        for cls, cnt in zip(unique, counts):
            print(f"      {self.class_names[cls]}({cls}): {cnt:,}ê°œ ({cnt/len(y)*100:.1f}%)")
        
        return X, y, df_clean
    
    def train(self, X, y, test_size=0.2, val_size=0.1):
        """ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ"""
        print(f"\n{'='*80}")
        print(f"ğŸ¯ [{self.target_variable}] ì•™ìƒë¸” ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ")
        print(f"{'='*80}")
        
        # Train / Val / Test ë¶„í• 
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=test_size, random_state=self.random_state, stratify=y
        )
        
        val_ratio = val_size / (1 - test_size)
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_ratio, random_state=self.random_state, stratify=y_temp
        )
        
        print(f"   ğŸ“Š Train: {len(X_train):,} | Val: {len(X_val):,} | Test: {len(X_test):,}")
        
        # ìŠ¤ì¼€ì¼ë§
        X_train_scaled = self.scaler_X.fit_transform(X_train)
        X_val_scaled = self.scaler_X.transform(X_val)
        X_test_scaled = self.scaler_X.transform(X_test)
        
        # 3ê°œ ëª¨ë¸ ìƒì„±
        rf_model = RandomForestClassifier(
            n_estimators=300,
            max_depth=15,
            min_samples_split=10,
            min_samples_leaf=5,
            random_state=self.random_state,
            n_jobs=-1
        )
        
        xgb_model = xgb.XGBClassifier(
            n_estimators=300,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=self.random_state,
            eval_metric='mlogloss',
            use_label_encoder=False
        )
        
        lgb_model = lgb.LGBMClassifier(
            n_estimators=300,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=self.random_state,
            verbose=-1
        )
        
        # Voting Ensemble
        self.model = VotingClassifier(
            estimators=[
                ('rf', rf_model),
                ('xgb', xgb_model),
                ('lgb', lgb_model)
            ],
            voting='soft',  # í™•ë¥  ê¸°ë°˜ íˆ¬í‘œ
            n_jobs=-1
        )
        
        print(f"\n   ğŸ”„ ì•™ìƒë¸” í•™ìŠµ ì¤‘ (RF + XGBoost + LightGBM)...")
        
        # í•™ìŠµ
        self.model.fit(X_train_scaled, y_train)
        
        # í‰ê°€
        self._evaluate(X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test)
        
        return X_test_scaled, y_test
    
    def _evaluate(self, X_train, y_train, X_val, y_val, X_test, y_test):
        """ëª¨ë¸ í‰ê°€"""
        print(f"\n   ğŸ“ˆ ì„±ëŠ¥ í‰ê°€:")
        
        datasets = {
            'Train': (X_train, y_train),
            'Val': (X_val, y_val),
            'Test': (X_test, y_test)
        }
        
        for name, (X, y) in datasets.items():
            y_pred = self.model.predict(X)
            y_pred_proba = self.model.predict_proba(X)
            
            accuracy = accuracy_score(y, y_pred)
            precision, recall, f1, _ = precision_recall_fscore_support(y, y_pred, average='weighted')
            
            self.metrics[name] = {
                'Accuracy': accuracy,
                'Precision': precision,
                'Recall': recall,
                'F1-Score': f1
            }
            
            print(f"\n      [{name}]")
            print(f"         Accuracy = {accuracy:.4f}")
            print(f"         Precision = {precision:.4f}")
            print(f"         Recall = {recall:.4f}")
            print(f"         F1-Score = {f1:.4f}")
    
    def plot_confusion_matrix(self, X_test, y_test):
        """í˜¼ë™ í–‰ë ¬ ì‹œê°í™”"""
        y_pred = self.model.predict(X_test)
        cm = confusion_matrix(y_test, y_pred)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=self.class_names,
                   yticklabels=self.class_names)
        plt.xlabel('ì˜ˆì¸¡', fontsize=12)
        plt.ylabel('ì‹¤ì œ', fontsize=12)
        plt.title(f'{self.target_variable} í˜¼ë™ í–‰ë ¬ (Accuracy={self.metrics["Test"]["Accuracy"]:.4f})', 
                 fontsize=14, fontweight='bold')
        
        output_path = f'./result/ensemble_{self.target_variable}_confusion_matrix.png'
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        print(f"   ğŸ’¾ í˜¼ë™ í–‰ë ¬ ì €ì¥: {output_path}")
        plt.close()
    
    def plot_feature_importance(self, top_n=20):
        """íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™” (Random Forest ê¸°ì¤€)"""
        rf_model = self.model.named_estimators_['rf']
        importance = rf_model.feature_importances_
        indices = np.argsort(importance)[::-1][:top_n]
        
        plt.figure(figsize=(12, 8))
        plt.barh(range(top_n), importance[indices])
        plt.yticks(range(top_n), [self.feature_names[i] for i in indices])
        plt.xlabel('Feature Importance', fontsize=12)
        plt.title(f'{self.target_variable} íŠ¹ì„± ì¤‘ìš”ë„ (Top {top_n})', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3, axis='x')
        
        output_path = f'./result/ensemble_{self.target_variable}_feature_importance.png'
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        print(f"   ğŸ’¾ íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥: {output_path}")
        plt.close()
    
    def save_model(self, output_dir='./result/models'):
        """ëª¨ë¸ ì €ì¥"""
        os.makedirs(output_dir, exist_ok=True)
        
        model_path = os.path.join(output_dir, f'ensemble_{self.target_variable}.pkl')
        scaler_path = os.path.join(output_dir, f'scaler_X_{self.target_variable}_clf.pkl')
        
        joblib.dump(self.model, model_path)
        joblib.dump(self.scaler_X, scaler_path)
        
        print(f"\n   ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_path}")
        print(f"   ğŸ’¾ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {scaler_path}")


def train_all_targets(data_path='../data/ver2_paired_visits.csv'):
    """ëª¨ë“  ê±´ê°•ì§€í‘œì— ëŒ€í•´ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ"""
    print("\n" + "="*80)
    print("ğŸš€ Ver2 ì•™ìƒë¸” ë¶„ë¥˜ ëª¨ë¸ ì „ì²´ í•™ìŠµ")
    print("="*80)
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(data_path)
    print(f"\nâœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,}ê°œ ìƒ˜í”Œ")
    
    # ê±´ê°•ì§€í‘œ ëª©ë¡
    health_indicators = [
        'ì²´ì¤‘', 'ì²´ì§ˆëŸ‰ì§€ìˆ˜', 'í—ˆë¦¬ë‘˜ë ˆ(WAIST)', 'SBP', 'DBP', 'TG'
    ]
    
    results = {}
    
    for indicator in health_indicators:
        try:
            print(f"\n{'='*80}")
            print(f"ğŸ¯ [{indicator}] ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
            print(f"{'='*80}")
            
            # ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
            model = EnsembleClassifier(indicator)
            X, y, df_clean = model.prepare_data(df)
            X_test, y_test = model.train(X, y)
            
            # ì‹œê°í™”
            model.plot_confusion_matrix(X_test, y_test)
            model.plot_feature_importance()
            
            # ëª¨ë¸ ì €ì¥
            model.save_model()
            
            # ê²°ê³¼ ì €ì¥
            results[indicator] = model.metrics['Test']
            
            print(f"\nâœ… [{indicator}] ì™„ë£Œ!")
            
        except Exception as e:
            print(f"\nâŒ [{indicator}] ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            results[indicator] = None
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    print("\n" + "="*80)
    print("ğŸ“Š ì•™ìƒë¸” ë¶„ë¥˜ ëª¨ë¸ ì „ì²´ ê²°ê³¼ ìš”ì•½")
    print("="*80)
    
    valid_results = {k: v for k, v in results.items() if v is not None}
    
    if len(valid_results) > 0:
        results_df = pd.DataFrame(valid_results).T
        print("\n", results_df.round(4))
        
        # ê²°ê³¼ ì €ì¥
        output_csv = './result/ensemble_all_results.csv'
        Path(output_csv).parent.mkdir(parents=True, exist_ok=True)
        results_df.to_csv(output_csv)
        print(f"\nğŸ’¾ ì „ì²´ ê²°ê³¼ ì €ì¥: {output_csv}")
    else:
        print("\nâš ï¸ ëª¨ë“  ì§€í‘œì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        results_df = pd.DataFrame()
    
    return results_df


if __name__ == '__main__':
    # ì „ì²´ í•™ìŠµ
    results = train_all_targets()
