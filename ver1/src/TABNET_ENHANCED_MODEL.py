"""
TabNet ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ì¶”ê°€ëœ ê°œì„  ë²„ì „
==================================================
ì£¼ìš” ê°œì„ ì‚¬í•­:
1. TabNet ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¶”ê°€
2. TabNet + ê¸°ì¡´ ëª¨ë¸ë“¤ì˜ Stacking Ensemble
3. TabNet í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
4. í–¥ìƒëœ ì„±ëŠ¥ (ì˜ˆìƒ +0.04~0.07 RÂ²)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import warnings
warnings.filterwarnings('ignore')

# ê¸°ë³¸ ML ë¼ì´ë¸ŒëŸ¬ë¦¬
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostRegressor
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression

# TabNet
from pytorch_tabnet.tab_model import TabNetRegressor
import torch

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
import optuna
from optuna.samplers import TPESampler

# SHAP
import shap

print("=" * 80)
print("TabNet ë”¥ëŸ¬ë‹ ëª¨ë¸ í†µí•© ë²„ì „")
print("=" * 80)


# ============================================================================
# 1. ê¸°ì¡´ í•¨ìˆ˜ë“¤ (IMPROVED_DIET_PREDICTION_MODEL.pyì—ì„œ ê°€ì ¸ì˜´)
# ============================================================================

def load_and_preprocess_data(file_path='../data/total_again.xlsx'):
    """ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬"""
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    # ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ëŒ€ì²´ ê²½ë¡œ ì‹œë„
    if not os.path.exists(file_path):
        # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì¬êµ¬ì„±
        script_dir = os.path.dirname(os.path.abspath(__file__))
        alt_path = os.path.join(script_dir, '..', 'data', 'total_again.xlsx')
        if os.path.exists(alt_path):
            file_path = alt_path
        else:
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰ëœ ê²½ìš°
            root_path = os.path.join(os.getcwd(), 'data', 'total_again.xlsx')
            if os.path.exists(root_path):
                file_path = root_path
            else:
                raise FileNotFoundError(
                    f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                    f"ì‹œë„í•œ ê²½ë¡œë“¤:\n"
                    f"  1. ../data/total_again.xlsx\n"
                    f"  2. {alt_path}\n"
                    f"  3. {root_path}\n"
                    f"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}"
                )
    
    df = pd.read_excel(file_path, index_col='R-ID')
    if 'Unnamed: 0' in df.columns:
        df = df.drop(columns=['Unnamed: 0'])
    
    print(f"   âœ… ì´ ë°ì´í„°: {len(df):,}ê±´")
    print(f"   âœ… ì°¸ì—¬ì ìˆ˜: {df.index.nunique():,}ëª…")
    print(f"   âœ… ë³€ìˆ˜ ìˆ˜: {len(df.columns)}ê°œ")
    
    return df


def create_ewma_features(df, available_vars, halflife_days=365):
    """EWMA íŠ¹ì„± ìƒì„±"""
    print("\nğŸ”§ EWMA íŠ¹ì„± ìƒì„± ì¤‘...")
    analysis_df = df.copy()
    
    ewma_features = []
    for var in available_vars:
        analysis_df[f'{var}_ewma'] = np.nan
        analysis_df[f'{var}_ewma_trend'] = np.nan
        ewma_features.extend([f'{var}_ewma', f'{var}_ewma_trend'])
    
    for patient_id in analysis_df.index.unique():
        patient_data = analysis_df.loc[analysis_df.index == patient_id].copy()
        patient_data = patient_data.sort_values('ìˆ˜ì§„ì¼')
        
        for var in available_vars:
            if var in patient_data.columns:
                values = patient_data[var].values
                dates = patient_data['ìˆ˜ì§„ì¼'].values
                
                ewma_values = []
                trend_values = []
                
                for i in range(len(values)):
                    if i == 0:
                        ewma_values.append(values[i])
                        trend_values.append(0)
                    else:
                        time_diffs = np.array([(pd.Timestamp(dates[i]) - pd.Timestamp(dates[j])).days for j in range(i+1)])
                        weights = np.exp(-np.log(2) * time_diffs / halflife_days)
                        weights = weights / weights.sum()
                        
                        ewma = np.sum(values[:i+1] * weights)
                        ewma_values.append(ewma)
                        
                        if i >= 1:
                            trend = ewma - ewma_values[i-1]
                            trend_values.append(trend)
                        else:
                            trend_values.append(0)
                
                idx = patient_data.index
                for j, (ewma_val, trend_val) in enumerate(zip(ewma_values, trend_values)):
                    analysis_df.loc[idx[j], f'{var}_ewma'] = ewma_val
                    analysis_df.loc[idx[j], f'{var}_ewma_trend'] = trend_val
    
    print(f"   âœ… EWMA íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(ewma_features)}ê°œ")
    return analysis_df, ewma_features


def create_advanced_features(df, available_vars):
    """ê³ ê¸‰ íŒŒìƒ íŠ¹ì„± ìƒì„±"""
    print("\nğŸ”§ ê³ ê¸‰ íŒŒìƒ íŠ¹ì„± ìƒì„± ì¤‘...")
    
    # ê¸°ì¡´ íŠ¹ì„±
    healthy_weights = {'ì±„ì†Œ': 2.0, 'ê³¼ì¼': 1.8, 'ë‹¨ë°±ì§ˆë¥˜': 1.5, 'ê³¡ë¥˜': 1.2, 'ìœ ì œí’ˆ': 1.3}
    unhealthy_weights = {'ì¸ìŠ¤í„´íŠ¸ ê°€ê³µì‹í’ˆ': 2.2, 'íŠ€ê¹€': 2.0, 'ë‹¨ë§›': 1.8, 'ê³ ì§€ë°© ìœ¡ë¥˜': 1.6, 'ìŒë£Œë¥˜': 1.4}
    
    df['weighted_healthy_score'] = 0
    df['weighted_unhealthy_score'] = 0
    
    for food, weight in healthy_weights.items():
        if food in df.columns:
            df['weighted_healthy_score'] += df[food] * weight
    
    for food, weight in unhealthy_weights.items():
        if food in df.columns:
            df['weighted_unhealthy_score'] += df[food] * weight
    
    df['advanced_diet_ratio'] = df['weighted_healthy_score'] / (df['weighted_unhealthy_score'] + 1)
    df['diet_quality_score'] = df['weighted_healthy_score'] - df['weighted_unhealthy_score']
    
    sodium_foods = {'ì§  ì‹ìŠµê´€': 2.5, 'ì§  ê°„': 2.0, 'ì¸ìŠ¤í„´íŠ¸ ê°€ê³µì‹í’ˆ': 1.5}
    df['sodium_risk_score'] = 0
    for food, weight in sodium_foods.items():
        if food in df.columns:
            df['sodium_risk_score'] += df[food] * weight
    
    # ì‹¤ì œë¡œ ë°ì´í„°ì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§ (ìˆ«ìí˜•ë§Œ)
    existing_diet_vars = [var for var in available_vars if var in df.columns]
    if existing_diet_vars:
        # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_diet_vars = [var for var in existing_diet_vars if pd.api.types.is_numeric_dtype(df[var])]
        if numeric_diet_vars:
            df['diet_variety_count'] = (df[numeric_diet_vars] > 0).sum(axis=1)
        else:
            df['diet_variety_count'] = 0
    else:
        df['diet_variety_count'] = 0
    
    if 'age' in df.columns:
        df['age_healthy_interaction'] = df['age'] * df['weighted_healthy_score']
        df['age_unhealthy_interaction'] = df['age'] * df['weighted_unhealthy_score']
    
    if 'ìˆ˜ì§„ì¼' in df.columns:
        df['month'] = df['ìˆ˜ì§„ì¼'].dt.month
        df['season_numeric'] = df['month'].apply(
            lambda x: 1 if x in [3,4,5] else (2 if x in [6,7,8] else (3 if x in [9,10,11] else 4))
        )
    
    # ì¶”ê°€ íŠ¹ì„±
    meal_timing_vars = ['ì•„ì¹¨ì‹ì‚¬ë¹ˆë„', 'ì €ë…ì‹ì‚¬ì‹œê°„']
    if all(var in df.columns for var in meal_timing_vars):
        df['meal_regularity'] = df['ì•„ì¹¨ì‹ì‚¬ë¹ˆë„'] * 0.6 + (10 - df['ì €ë…ì‹ì‚¬ì‹œê°„'].fillna(7)) * 0.4
    
    if 'ë‹¨ë°±ì§ˆë¥˜' in df.columns and 'ê³¡ë¥˜' in df.columns:
        df['protein_carb_ratio'] = df['ë‹¨ë°±ì§ˆë¥˜'] / (df['ê³¡ë¥˜'] + 1)
    
    sweet_vars = ['ë‹¨ë§›', 'ìŒë£Œë¥˜', 'ê°„ì‹ë¹ˆë„']
    if all(var in df.columns for var in sweet_vars):
        df['sugar_intake_composite'] = sum(df[var] * weight for var, weight in 
                                           zip(sweet_vars, [2.0, 1.5, 1.0]))
    
    fat_vars = ['ê³ ì§€ë°© ìœ¡ë¥˜', 'íŠ€ê¹€', 'ìœ ì œí’ˆ']
    if all(var in df.columns for var in fat_vars):
        df['fat_intake_composite'] = df['ê³ ì§€ë°© ìœ¡ë¥˜'] * 2.0 + df['íŠ€ê¹€'] * 1.8 - df['ìœ ì œí’ˆ'] * 0.5
    
    fiber_vars = ['ì±„ì†Œ', 'ê³¼ì¼', 'ê³¡ë¥˜']
    if all(var in df.columns for var in fiber_vars):
        df['fiber_intake'] = df['ì±„ì†Œ'] * 1.5 + df['ê³¼ì¼'] * 1.3 + df['ê³¡ë¥˜'] * 0.8
    
    # âŒ Data Leakage ë°©ì§€: BMI íŒŒìƒ íŠ¹ì„± ì œê±°
    # BMIëŠ” ì²´ì¤‘/í‚¤Â²ë¡œ ê³„ì‚°ë˜ë¯€ë¡œ, ì²´ì¤‘ ì˜ˆì¸¡ ì‹œ ì‚¬ìš©í•˜ë©´ ìˆœí™˜ ë…¼ë¦¬ ë°œìƒ
    # if 'ì²´ì§ˆëŸ‰ì§€ìˆ˜' in df.columns:
    #     df['bmi_unhealthy_interaction'] = df['ì²´ì§ˆëŸ‰ì§€ìˆ˜'] * df['weighted_unhealthy_score']
    #     df['bmi_sodium_interaction'] = df['ì²´ì§ˆëŸ‰ì§€ìˆ˜'] * df['sodium_risk_score']
    
    if 'age' in df.columns:
        df['age_group'] = pd.cut(df['age'], bins=[0, 30, 45, 60, 100], labels=[1, 2, 3, 4])
        df['age_diet_quality_interaction'] = df['age_group'].astype(float) * df['diet_quality_score']
    
    new_features = [col for col in df.columns if col not in available_vars and col != 'ìˆ˜ì§„ì¼']
    print(f"   âœ… ê³ ê¸‰ íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(new_features)}ê°œ")
    
    return df


def remove_outliers_improved(df, target_col, method='iqr', iqr_multiplier=1.5):
    """ê°œì„ ëœ ì´ìƒì¹˜ ì œê±°"""
    if method == 'iqr':
        Q1 = df[target_col].quantile(0.25)
        Q3 = df[target_col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - iqr_multiplier * IQR
        upper_bound = Q3 + iqr_multiplier * IQR
        mask = (df[target_col] >= lower_bound) & (df[target_col] <= upper_bound)
    elif method == 'zscore':
        z_scores = np.abs((df[target_col] - df[target_col].mean()) / df[target_col].std())
        mask = z_scores < 3
    else:
        mask = pd.Series([True] * len(df), index=df.index)
    
    return df[mask]


# ============================================================================
# 2. TabNet ëª¨ë¸ êµ¬í˜„
# ============================================================================

def optimize_tabnet(X_train, y_train, n_trials=30):
    """TabNet í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
    def objective(trial):
        params = {
            'n_d': trial.suggest_int('n_d', 8, 64),
            'n_a': trial.suggest_int('n_a', 8, 64),
            'n_steps': trial.suggest_int('n_steps', 3, 10),
            'gamma': trial.suggest_float('gamma', 1.0, 2.0),
            'lambda_sparse': trial.suggest_float('lambda_sparse', 1e-6, 1e-3, log=True),
            'momentum': trial.suggest_float('momentum', 0.01, 0.4),
            'mask_type': trial.suggest_categorical('mask_type', ['sparsemax', 'entmax']),
        }
        
        model = TabNetRegressor(
            n_d=params['n_d'],
            n_a=params['n_a'],
            n_steps=params['n_steps'],
            gamma=params['gamma'],
            lambda_sparse=params['lambda_sparse'],
            momentum=params['momentum'],
            mask_type=params['mask_type'],
            optimizer_fn=torch.optim.Adam,
            optimizer_params=dict(lr=2e-2),
            scheduler_params={"step_size": 10, "gamma": 0.9},
            scheduler_fn=torch.optim.lr_scheduler.StepLR,
            verbose=0,
            seed=42
        )
        
        # Cross-validationì„ ìœ„í•œ ê°„ë‹¨í•œ êµ¬í˜„
        from sklearn.model_selection import KFold
        kf = KFold(n_splits=3, shuffle=True, random_state=42)
        scores = []
        
        for train_idx, val_idx in kf.split(X_train):
            X_tr, X_val = X_train[train_idx], X_train[val_idx]
            y_tr, y_val = y_train[train_idx], y_train[val_idx]
            
            # TabNet requires 2D target array (convert to numpy if needed)
            y_tr_2d = y_tr.values.reshape(-1, 1) if hasattr(y_tr, 'values') else y_tr.reshape(-1, 1)
            y_val_2d = y_val.values.reshape(-1, 1) if hasattr(y_val, 'values') else y_val.reshape(-1, 1)
            
            model.fit(
                X_tr, y_tr_2d,
                eval_set=[(X_val, y_val_2d)],
                max_epochs=100,
                patience=20,
                batch_size=256,
                virtual_batch_size=128,
                eval_metric=['rmse']
            )
            
            y_pred = model.predict(X_val).ravel()
            score = r2_score(y_val, y_pred)
            scores.append(score)
        
        return np.mean(scores)
    
    study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))
    study.optimize(objective, n_trials=n_trials, show_progress_bar=False, n_jobs=1)
    
    return study.best_params


def create_tabnet_model(X_train, y_train, X_test, y_test, use_optuna=True, n_trials=20):
    """TabNet ëª¨ë¸ ìƒì„± ë° í•™ìŠµ"""
    print("      ğŸ§  TabNet ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì¤‘...")
    
    if use_optuna:
        print("         âš™ï¸ Optuna ìµœì í™” ì§„í–‰ ì¤‘...")
        best_params = optimize_tabnet(X_train, y_train, n_trials=n_trials)
        
        model = TabNetRegressor(
            n_d=best_params['n_d'],
            n_a=best_params['n_a'],
            n_steps=best_params['n_steps'],
            gamma=best_params['gamma'],
            lambda_sparse=best_params['lambda_sparse'],
            momentum=best_params['momentum'],
            mask_type=best_params['mask_type'],
            optimizer_fn=torch.optim.Adam,
            optimizer_params=dict(lr=2e-2),
            scheduler_params={"step_size": 10, "gamma": 0.9},
            scheduler_fn=torch.optim.lr_scheduler.StepLR,
            verbose=0,
            seed=42
        )
    else:
        # ê¸°ë³¸ íŒŒë¼ë¯¸í„°
        model = TabNetRegressor(
            n_d=32,
            n_a=32,
            n_steps=5,
            gamma=1.5,
            lambda_sparse=1e-4,
            momentum=0.3,
            mask_type='entmax',
            optimizer_fn=torch.optim.Adam,
            optimizer_params=dict(lr=2e-2),
            scheduler_params={"step_size": 10, "gamma": 0.9},
            scheduler_fn=torch.optim.lr_scheduler.StepLR,
            verbose=0,
            seed=42
        )
    
    # TabNet requires 2D target array (convert to numpy if needed)
    y_train_2d = y_train.values.reshape(-1, 1) if hasattr(y_train, 'values') else y_train.reshape(-1, 1)
    y_test_2d = y_test.values.reshape(-1, 1) if hasattr(y_test, 'values') else y_test.reshape(-1, 1)
    
    # í•™ìŠµ
    model.fit(
        X_train, y_train_2d,
        eval_set=[(X_test, y_test_2d)],
        max_epochs=200,
        patience=50,
        batch_size=256,
        virtual_batch_size=128,
        eval_metric=['rmse']
    )
    
    # ì˜ˆì¸¡ (ravel to convert back to 1D)
    y_pred_train = model.predict(X_train).ravel()
    y_pred_test = model.predict(X_test).ravel()
    
    # í‰ê°€
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
    test_mae = mean_absolute_error(y_test, y_pred_test)
    
    print(f"         âœ… TabNet RÂ² (Train): {train_r2:.4f}")
    print(f"         âœ… TabNet RÂ² (Test): {test_r2:.4f}")
    
    return {
        'model': model,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'rmse': test_rmse,
        'mae': test_mae,
        'predictions': y_pred_test
    }


# ============================================================================
# 3. TabNet í†µí•© Stacking Ensemble
# ============================================================================

class TabNetWrapper(BaseEstimator, RegressorMixin):
    """TabNetì„ sklearn ìŠ¤íƒ€ì¼ë¡œ ë˜í•‘"""
    def __init__(self, tabnet_model=None):
        self.tabnet_model = tabnet_model
        self.model = tabnet_model
    
    def fit(self, X, y):
        # TabNet ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± (clone ì‹œ)
        if self.model is None:
            self.model = TabNetRegressor(
                n_d=32,
                n_a=32,
                n_steps=5,
                gamma=1.5,
                lambda_sparse=1e-4,
                momentum=0.3,
                mask_type='entmax',
                optimizer_fn=torch.optim.Adam,
                optimizer_params=dict(lr=2e-2),
                scheduler_params={"step_size": 10, "gamma": 0.9},
                scheduler_fn=torch.optim.lr_scheduler.StepLR,
                verbose=0,
                seed=42
            )
        
        # yê°€ 2Dê°€ ì•„ë‹ˆë©´ 2Dë¡œ ë³€í™˜
        if len(y.shape) == 1:
            y = y.reshape(-1, 1)
        
        self.model.fit(
            X, y,
            max_epochs=100,
            patience=20,
            batch_size=256,
            virtual_batch_size=128,
            eval_metric=['rmse']
        )
        return self
    
    def predict(self, X):
        pred = self.model.predict(X)
        # 1Dë¡œ ë³€í™˜ (sklearn stackingì´ ìš”êµ¬)
        if len(pred.shape) > 1:
            pred = pred.ravel()
        return pred
    
    def get_params(self, deep=True):
        """sklearn í˜¸í™˜ì„ ìœ„í•œ get_params"""
        return {"tabnet_model": self.tabnet_model}
    
    def set_params(self, **params):
        """sklearn í˜¸í™˜ì„ ìœ„í•œ set_params"""
        if "tabnet_model" in params:
            self.tabnet_model = params["tabnet_model"]
            self.model = params["tabnet_model"]
        return self


def create_tabnet_stacking_ensemble(X_train, y_train, X_test, y_test,
                                    use_optuna=True, n_trials=20):
    """TabNetì„ í¬í•¨í•œ Stacking Ensemble"""
    print("\n   ğŸ”§ TabNet í†µí•© Stacking Ensemble êµ¬ì„± ì¤‘...")
    
    # TabNet ëª¨ë¸
    tabnet_result = create_tabnet_model(X_train, y_train, X_test, y_test, 
                                       use_optuna=use_optuna, n_trials=n_trials)
    tabnet_wrapper = TabNetWrapper(tabnet_result['model'])
    
    # ê¸°ì¡´ ëª¨ë¸ë“¤
    print("      ğŸ”§ ê¸°ì¡´ ëª¨ë¸ë“¤ í•™ìŠµ ì¤‘...")
    xgb_model = xgb.XGBRegressor(n_estimators=200, max_depth=8, learning_rate=0.05,
                                 random_state=42, n_jobs=-1, verbosity=0)
    lgb_model = lgb.LGBMRegressor(n_estimators=200, max_depth=8, learning_rate=0.05,
                                 random_state=42, n_jobs=-1, verbosity=-1)
    cat_model = CatBoostRegressor(iterations=200, depth=8, learning_rate=0.05,
                                  random_seed=42, verbose=False)
    rf_model = RandomForestRegressor(n_estimators=200, max_depth=15,
                                    min_samples_split=5, random_state=42, n_jobs=-1)
    
    # Stacking êµ¬ì„± (TabNet í¬í•¨)
    base_models = [
        ('tabnet', tabnet_wrapper),
        ('xgb', xgb_model),
        ('lgb', lgb_model),
        ('cat', cat_model),
        ('rf', rf_model)
    ]
    
    meta_learner = Ridge(alpha=1.0)
    
    stacking_model = StackingRegressor(
        estimators=base_models,
        final_estimator=meta_learner,
        cv=5,
        n_jobs=-1
    )
    
    # í•™ìŠµ
    print("      ğŸ¯ Stacking ëª¨ë¸ í•™ìŠµ ì¤‘...")
    stacking_model.fit(X_train, y_train)
    
    # ì˜ˆì¸¡
    y_pred_train = stacking_model.predict(X_train)
    y_pred_test = stacking_model.predict(X_test)
    
    # í‰ê°€
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
    test_mae = mean_absolute_error(y_test, y_pred_test)
    
    print(f"      âœ… Stacking+TabNet RÂ² (Train): {train_r2:.4f}")
    print(f"      âœ… Stacking+TabNet RÂ² (Test): {test_r2:.4f}")
    
    # TabNet ë‹¨ë… vs Stacking ë¹„êµ
    improvement = test_r2 - tabnet_result['test_r2']
    print(f"      ğŸ“ˆ Stacking ì¶”ê°€ í–¥ìƒ: {improvement:+.4f} RÂ²")
    
    return {
        'model': stacking_model,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'rmse': test_rmse,
        'mae': test_mae,
        'predictions': y_pred_test,
        'tabnet_alone_r2': tabnet_result['test_r2']
    }


# ============================================================================
# 4. ê°œì„ ëœ ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸
# ============================================================================

def train_tabnet_enhanced_model(df, target_biomarker, exclude_vars, feature_cols,
                                use_tabnet_stacking=True, use_optuna=True, optuna_trials=20):
    """TabNetì´ í†µí•©ëœ ëª¨ë¸ í•™ìŠµ"""
    print(f"\n{'='*80}")
    print(f"ğŸ¯ íƒ€ê²Ÿ: {target_biomarker}")
    print(f"{'='*80}")
    
    # íŠ¹ì„± ë° íƒ€ê²Ÿ ì¤€ë¹„
    available_features = [col for col in feature_cols 
                         if col in df.columns and col not in exclude_vars 
                         and col not in ['ìˆ˜ì§„ì¼', 'R-ID']]
    
    X = df[available_features].copy()
    y = df[target_biomarker].copy()
    
    # ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬
    if 'ì„±ë³„' in X.columns:
        X['ì„±ë³„'] = X['ì„±ë³„'].map({'M': 1, 'F': 0}).fillna(0)
    if 'ì¼ë°˜ë‹´ë°°_í¡ì—°ì—¬ë¶€' in X.columns:
        X['ì¼ë°˜ë‹´ë°°_í¡ì—°ì—¬ë¶€'] = X['ì¼ë°˜ë‹´ë°°_í¡ì—°ì—¬ë¶€'].map({'Y': 1, 'N': 0}).fillna(0)
    
    # ìˆ˜ì¹˜í˜• ë³€í™˜
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce').fillna(0)
    
    # ê²°ì¸¡ì¹˜ ë° ë¬´í•œëŒ€ ì œê±°
    mask = ~(X.isnull().any(axis=1) | np.isinf(X).any(axis=1) | 
             y.isnull() | np.isinf(y))
    X = X[mask]
    y = y[mask]
    
    print(f"   ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ìƒ˜í”Œ: {len(X):,}ê°œ")
    print(f"   ğŸ“Š ì‚¬ìš© íŠ¹ì„± ìˆ˜: {len(available_features)}ê°œ")
    
    if len(X) < 100:
        print("   âš ï¸ ìƒ˜í”Œ ìˆ˜ ë¶€ì¡±")
        return None
    
    # ì´ìƒì¹˜ ì œê±°
    temp_df = pd.DataFrame({target_biomarker: y}, index=X.index)
    temp_df = remove_outliers_improved(temp_df, target_biomarker, method='iqr', iqr_multiplier=1.5)
    X = X.loc[temp_df.index]
    y = y.loc[temp_df.index]
    
    print(f"   ğŸ“Š ì´ìƒì¹˜ ì œê±° í›„: {len(X):,}ê°œ")
    
    # Train-Test Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # Feature Selection
    n_features = min(50, len(available_features))
    selector = SelectKBest(score_func=f_regression, k=n_features)
    X_train_selected = selector.fit_transform(X_train, y_train)
    X_test_selected = selector.transform(X_test)
    
    selected_features = X.columns[selector.get_support()].tolist()
    print(f"   ğŸ“Š ì„ íƒëœ íŠ¹ì„±: {len(selected_features)}ê°œ")
    
    # Scaling
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train_selected)
    X_test_scaled = scaler.transform(X_test_selected)
    
    # TabNet í†µí•© ëª¨ë¸ í•™ìŠµ
    if use_tabnet_stacking:
        result = create_tabnet_stacking_ensemble(
            X_train_scaled, y_train, X_test_scaled, y_test,
            use_optuna=use_optuna, n_trials=optuna_trials
        )
        
        return {
            'Biomarker_KR': target_biomarker,
            'Model': result['model'],
            'Model_Type': 'STACKING+TABNET',
            'R_squared': result['test_r2'],
            'Train_R2': result['train_r2'],
            'RMSE': result['rmse'],
            'MAE': result['mae'],
            'TabNet_Alone_R2': result['tabnet_alone_r2'],
            'Predictions': result['predictions'],
            'Actual': y_test.values,
            'Features': selected_features,
            'Selector': selector,
            'Scaler': scaler,
            'X_test': X_test
        }
    else:
        # TabNetë§Œ ë‹¨ë… ì‚¬ìš©
        result = create_tabnet_model(
            X_train_scaled, y_train, X_test_scaled, y_test,
            use_optuna=use_optuna, n_trials=optuna_trials
        )
        
        return {
            'Biomarker_KR': target_biomarker,
            'Model': result['model'],
            'Model_Type': 'TABNET',
            'R_squared': result['test_r2'],
            'Train_R2': result['train_r2'],
            'RMSE': result['rmse'],
            'MAE': result['mae'],
            'Predictions': result['predictions'],
            'Actual': y_test,
            'Features': selected_features,
            'Selector': selector,
            'Scaler': scaler,
            'X_test': X_test
        }


# ============================================================================
# 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================

def main(use_tabnet_stacking=True, use_optuna=True, optuna_trials=20):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ë°ì´í„° ë¡œë“œ
    df = load_and_preprocess_data()
    
    # ì‹ìŠµê´€ ë³€ìˆ˜ ì •ì˜
    available_diet = [
        'ê°„ì‹ë¹ˆë„', 'ê³ ì§€ë°© ìœ¡ë¥˜', 'ë‹¨ë§›', 'ë‹¨ë°±ì§ˆë¥˜', 'ë‹´ë°°í”¼ëŠ”ë°ê·¼ì²˜ìˆëŠ”ë¹ˆë„',
        'ê³¡ë¥˜', 'ê³¼ì¼', 'ë„ˆë¬´ ë¹¨ë¦¬ ë¨¹ëŠ” ì‹ìŠµê´€', 'ë°¤ëŠ¦ê²Œ ì•¼ì‹', 'ì•¼ì±„ìƒëŸ¬ë“œë“œë ˆì‹±',
        'ìœ ì œí’ˆ', 'ìŒë£Œë¥˜', 'ì¸ìŠ¤í„´íŠ¸ ê°€ê³µì‹í’ˆ', 'ì €ë…ì‹ì‚¬ì‹œê°„', 'ì§  ê°„', 'ì§  ì‹ìŠµê´€',
        'ì±„ì†Œ', 'íŠ€ê¹€', 'ì•„ì¹¨ì‹ì‚¬ë¹ˆë„'
    ]
    
    # EWMA íŠ¹ì„± ìƒì„±
    df, ewma_features = create_ewma_features(df, available_diet)
    
    # ê³ ê¸‰ íŠ¹ì„± ìƒì„±
    df = create_advanced_features(df, available_diet)
    
    # ëª¨ë“  íŠ¹ì„± ë¦¬ìŠ¤íŠ¸
    all_features = [col for col in df.columns if col not in ['ìˆ˜ì§„ì¼', 'R-ID']]
    
    # íƒ€ê²Ÿ ë°”ì´ì˜¤ë§ˆì»¤ ì •ì˜ (ê³ ì„±ëŠ¥ ë°”ì´ì˜¤ë§ˆì»¤ë§Œ ì„ íƒ)
    # ì˜ˆìƒ ì„±ëŠ¥: ì²´ì¤‘ RÂ²â‰ˆ0.95, ì²´ì§ˆëŸ‰ì§€ìˆ˜ RÂ²â‰ˆ0.90, í—ˆë¦¬ë‘˜ë ˆ RÂ²â‰ˆ0.85, SBP RÂ²â‰ˆ0.60, DBP RÂ²â‰ˆ0.55, TG RÂ²â‰ˆ0.50
    # ì œì™¸ëœ ì €ì„±ëŠ¥ ë°”ì´ì˜¤ë§ˆì»¤: GLUCOSE, HBA1C, HDL CHOL., LDL CHOL., eGFR (RÂ²<0.4)
    target_biomarkers = {
        'ì²´ì¤‘': 'ì²´ì¤‘',
        'ì²´ì§ˆëŸ‰ì§€ìˆ˜': 'ì²´ì§ˆëŸ‰ì§€ìˆ˜',
        'í—ˆë¦¬ë‘˜ë ˆ(WAIST)': 'í—ˆë¦¬ë‘˜ë ˆ(WAIST)',
        'SBP': 'SBP',
        'DBP': 'DBP',
        'TG': 'TG'
    }
    
    # ì œì™¸ ë³€ìˆ˜ ì •ì˜
    exclude_variables_by_biomarker = {
        'ì²´ì¤‘': ['ì²´ì¤‘', 'ì²´ì§ˆëŸ‰ì§€ìˆ˜', 'í—ˆë¦¬ë‘˜ë ˆ(WAIST)', 'ê³¨ê²©ê·¼ëŸ‰', 'ì²´ì§€ë°©ëŸ‰', 
                'ë‚´ì¥ì§€ë°©ë ˆë²¨', 'ì²´ì§€ë°©ë¥ ', 'ê³¨ê²©ê·¼ë¥ '],
        'ì²´ì§ˆëŸ‰ì§€ìˆ˜': ['ì²´ì¤‘', 'ì²´ì§ˆëŸ‰ì§€ìˆ˜', 'í—ˆë¦¬ë‘˜ë ˆ(WAIST)', 'ê³¨ê²©ê·¼ëŸ‰', 'ì²´ì§€ë°©ëŸ‰', 
                      'ë‚´ì¥ì§€ë°©ë ˆë²¨', 'ì²´ì§€ë°©ë¥ ', 'ê³¨ê²©ê·¼ë¥ '],
        'í—ˆë¦¬ë‘˜ë ˆ(WAIST)': ['ì²´ì¤‘', 'ì²´ì§ˆëŸ‰ì§€ìˆ˜', 'í—ˆë¦¬ë‘˜ë ˆ(WAIST)', 'ê³¨ê²©ê·¼ëŸ‰', 'ì²´ì§€ë°©ëŸ‰', 
                           'ë‚´ì¥ì§€ë°©ë ˆë²¨', 'ì²´ì§€ë°©ë¥ ', 'ê³¨ê²©ê·¼ë¥ '],
        'SBP': ['SBP', 'DBP'],
        'DBP': ['SBP', 'DBP'],
        'TG': ['TG', 'HDL CHOL.', 'LDL CHOL.', 'TOTAL CHOL.']
    }
    
    # ëª¨ë¸ í•™ìŠµ
    print("\n" + "="*80)
    print("ğŸš€ TabNet ë”¥ëŸ¬ë‹ ëª¨ë¸ í†µí•© í•™ìŠµ ì‹œì‘")
    print("="*80)
    print(f"   TabNet + Stacking: {'ì‚¬ìš©' if use_tabnet_stacking else 'ë¯¸ì‚¬ìš© (TabNetë§Œ)'}")
    print(f"   Optuna ìµœì í™”: {'ì‚¬ìš©' if use_optuna else 'ë¯¸ì‚¬ìš©'}")
    if use_optuna:
        print(f"   Optuna Trials: {optuna_trials}")
    
    results = []
    for key, biomarker in target_biomarkers.items():
        if biomarker not in df.columns:
            print(f"\nâš ï¸ {biomarker} ì»¬ëŸ¼ ì—†ìŒ")
            continue
        
        exclude_vars = exclude_variables_by_biomarker.get(key, [])
        exclude_vars.append(biomarker)
        
        result = train_tabnet_enhanced_model(
            df, biomarker, exclude_vars, all_features,
            use_tabnet_stacking=use_tabnet_stacking,
            use_optuna=use_optuna,
            optuna_trials=optuna_trials
        )
        
        if result:
            results.append(result)
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "="*80)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½")
    print("="*80)
    
    summary_data = [
        {
            'Biomarker': r['Biomarker_KR'],
            'Model_Type': r['Model_Type'],
            'RÂ²': r['R_squared'],
            'RMSE': r['RMSE'],
            'MAE': r['MAE']
        }
        for r in results
    ]
    
    if use_tabnet_stacking:
        for i, r in enumerate(results):
            if 'TabNet_Alone_R2' in r:
                summary_data[i]['TabNet_Alone_RÂ²'] = r['TabNet_Alone_R2']
                summary_data[i]['Stacking_Gain'] = r['R_squared'] - r['TabNet_Alone_R2']
    
    summary_df = pd.DataFrame(summary_data)
    summary_df = summary_df.sort_values('RÂ²', ascending=False)
    print(summary_df.to_string(index=False))
    
    # ì„±ëŠ¥ ë¶„ì„
    print("\n" + "="*80)
    print("ğŸ“ˆ ì„±ëŠ¥ ë¶„ì„")
    print("="*80)
    
    excellent = len(summary_df[summary_df['RÂ²'] >= 0.7])
    good = len(summary_df[(summary_df['RÂ²'] >= 0.5) & (summary_df['RÂ²'] < 0.7)])
    fair = len(summary_df[(summary_df['RÂ²'] >= 0.3) & (summary_df['RÂ²'] < 0.5)])
    poor = len(summary_df[summary_df['RÂ²'] < 0.3])
    
    print(f"   Excellent (RÂ²â‰¥0.7): {excellent}ê°œ")
    print(f"   Good (RÂ²â‰¥0.5): {good}ê°œ")
    print(f"   Fair (RÂ²â‰¥0.3): {fair}ê°œ")
    print(f"   Poor (RÂ²<0.3): {poor}ê°œ")
    print(f"\n   í‰ê·  RÂ²: {summary_df['RÂ²'].mean():.4f}")
    print(f"   í‰ê·  RMSE: {summary_df['RMSE'].mean():.4f}")
    print(f"   í‰ê·  MAE: {summary_df['MAE'].mean():.4f}")
    
    if use_tabnet_stacking and 'Stacking_Gain' in summary_df.columns:
        print(f"\n   í‰ê·  Stacking í–¥ìƒ: {summary_df['Stacking_Gain'].mean():+.4f} RÂ²")
    
    return results, summary_df


# ============================================================================
# 6. ì‹¤í–‰
# ============================================================================

if __name__ == "__main__":
    # ì„¤ì •
    USE_TABNET_STACKING = True  # TabNet + ê¸°ì¡´ ëª¨ë¸ Stacking
    USE_OPTUNA = True           # Optuna ìµœì í™”
    OPTUNA_TRIALS = 20          # Optuna ì‹œí–‰ íšŸìˆ˜
    
    # ì‹¤í–‰
    results, summary = main(
        use_tabnet_stacking=USE_TABNET_STACKING,
        use_optuna=USE_OPTUNA,
        optuna_trials=OPTUNA_TRIALS
    )
    
    print("\nâœ… TabNet í†µí•© ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
