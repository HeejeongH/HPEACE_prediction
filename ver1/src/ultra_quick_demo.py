"""
ì´ˆê³ ì† ë°ëª¨ - EWMA ì—†ì´ ê¸°ë³¸ íŠ¹ì„±ë§Œ ì‚¬ìš©
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostRegressor
from sklearn.ensemble import RandomForestRegressor, StackingRegressor
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression

print("="*80)
print("âš¡ ì´ˆê³ ì† ë°ëª¨ - ê°œì„  ëª¨ë¸ ì„±ëŠ¥ í™•ì¸")
print("="*80)

# ë°ì´í„° ë¡œë“œ
print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
df = pd.read_excel('../data/total_again.xlsx', index_col='R-ID')
if 'Unnamed: 0' in df.columns:
    df = df.drop(columns=['Unnamed: 0'])

print(f"   âœ… ì´ ë°ì´í„°: {len(df):,}ê±´")

# ê°„ë‹¨í•œ íŠ¹ì„± ìƒì„±
print("\nğŸ”§ ê¸°ë³¸ íŠ¹ì„± ìƒì„± ì¤‘...")

# ê±´ê°•/ë¶ˆê±´ê°• ì ìˆ˜
healthy_foods = ['ì±„ì†Œ', 'ê³¼ì¼', 'ë‹¨ë°±ì§ˆë¥˜', 'ê³¡ë¥˜', 'ìœ ì œí’ˆ']
unhealthy_foods = ['ì¸ìŠ¤í„´íŠ¸ ê°€ê³µì‹í’ˆ', 'íŠ€ê¹€', 'ë‹¨ë§›', 'ê³ ì§€ë°© ìœ¡ë¥˜', 'ìŒë£Œë¥˜']

df['healthy_score'] = 0
for food in healthy_foods:
    if food in df.columns:
        df['healthy_score'] += df[food].fillna(0)

df['unhealthy_score'] = 0
for food in unhealthy_foods:
    if food in df.columns:
        df['unhealthy_score'] += df[food].fillna(0)

df['diet_ratio'] = df['healthy_score'] / (df['unhealthy_score'] + 1)

print(f"   âœ… íŠ¹ì„± ìƒì„± ì™„ë£Œ")

# íƒ€ê²Ÿ: ì²´ì¤‘
target = 'ì²´ì¤‘'
exclude_cols = ['ì²´ì¤‘', 'ì²´ì§ˆëŸ‰ì§€ìˆ˜', 'í—ˆë¦¬ë‘˜ë ˆ(WAIST)', 'ê³¨ê²©ê·¼ëŸ‰', 'ì²´ì§€ë°©ëŸ‰', 
                'ë‚´ì¥ì§€ë°©ë ˆë²¨', 'ì²´ì§€ë°©ë¥ ', 'ê³¨ê²©ê·¼ë¥ ', 'ìˆ˜ì§„ì¼', 'R-ID']

# íŠ¹ì„± ì„ íƒ
feature_cols = [col for col in df.columns if col not in exclude_cols]
X = df[feature_cols].copy()
y = df[target].copy()

# ë²”ì£¼í˜• ì²˜ë¦¬
if 'ì„±ë³„' in X.columns:
    X['ì„±ë³„'] = X['ì„±ë³„'].map({'M': 1, 'F': 0}).fillna(0)
if 'ì¼ë°˜ë‹´ë°°_í¡ì—°ì—¬ë¶€' in X.columns:
    X['ì¼ë°˜ë‹´ë°°_í¡ì—°ì—¬ë¶€'] = X['ì¼ë°˜ë‹´ë°°_í¡ì—°ì—¬ë¶€'].map({'Y': 1, 'N': 0}).fillna(0)

# ìˆ˜ì¹˜í˜• ë³€í™˜
for col in X.columns:
    X[col] = pd.to_numeric(X[col], errors='coerce').fillna(0)

# ê²°ì¸¡ì¹˜ ì œê±°
mask = ~(X.isnull().any(axis=1) | np.isinf(X).any(axis=1) | y.isnull() | np.isinf(y))
X = X[mask]
y = y[mask]

print(f"\nğŸ“Š ì‚¬ìš© ë°ì´í„°: {len(X):,}ê°œ ìƒ˜í”Œ, {len(feature_cols)}ê°œ íŠ¹ì„±")

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Selection
selector = SelectKBest(score_func=f_regression, k=min(30, len(feature_cols)))
X_train_selected = selector.fit_transform(X_train, y_train)
X_test_selected = selector.transform(X_test)

# Scaling
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train_selected)
X_test_scaled = scaler.transform(X_test_selected)

print(f"   âœ… ì„ íƒëœ íŠ¹ì„±: {X_train_selected.shape[1]}ê°œ")

print("\n" + "="*80)
print("ğŸ¯ ëª¨ë¸ í•™ìŠµ ì¤‘...")
print("="*80)

# ê°œë³„ ëª¨ë¸ í•™ìŠµ
models = {}

print("\n1ï¸âƒ£ XGBoost í•™ìŠµ...")
xgb_model = xgb.XGBRegressor(n_estimators=150, max_depth=8, learning_rate=0.05,
                             random_state=42, n_jobs=-1, verbosity=0)
xgb_model.fit(X_train_scaled, y_train)
xgb_pred = xgb_model.predict(X_test_scaled)
xgb_r2 = r2_score(y_test, xgb_pred)
print(f"   âœ… XGBoost RÂ²: {xgb_r2:.4f}")
models['XGBoost'] = {'model': xgb_model, 'r2': xgb_r2}

print("\n2ï¸âƒ£ LightGBM í•™ìŠµ...")
lgb_model = lgb.LGBMRegressor(n_estimators=150, max_depth=8, learning_rate=0.05,
                             random_state=42, n_jobs=-1, verbosity=-1)
lgb_model.fit(X_train_scaled, y_train)
lgb_pred = lgb_model.predict(X_test_scaled)
lgb_r2 = r2_score(y_test, lgb_pred)
print(f"   âœ… LightGBM RÂ²: {lgb_r2:.4f}")
models['LightGBM'] = {'model': lgb_model, 'r2': lgb_r2}

print("\n3ï¸âƒ£ CatBoost í•™ìŠµ...")
cat_model = CatBoostRegressor(iterations=150, depth=8, learning_rate=0.05,
                              random_seed=42, verbose=False)
cat_model.fit(X_train_scaled, y_train)
cat_pred = cat_model.predict(X_test_scaled)
cat_r2 = r2_score(y_test, cat_pred)
print(f"   âœ… CatBoost RÂ²: {cat_r2:.4f}")
models['CatBoost'] = {'model': cat_model, 'r2': cat_r2}

print("\n4ï¸âƒ£ Random Forest í•™ìŠµ...")
rf_model = RandomForestRegressor(n_estimators=150, max_depth=15,
                                min_samples_split=5, random_state=42, n_jobs=-1)
rf_model.fit(X_train_scaled, y_train)
rf_pred = rf_model.predict(X_test_scaled)
rf_r2 = r2_score(y_test, rf_pred)
print(f"   âœ… Random Forest RÂ²: {rf_r2:.4f}")
models['RandomForest'] = {'model': rf_model, 'r2': rf_r2}

print("\n5ï¸âƒ£ Stacking Ensemble í•™ìŠµ...")
base_models = [
    ('xgb', xgb_model),
    ('lgb', lgb_model),
    ('cat', cat_model),
    ('rf', rf_model)
]
meta_learner = Ridge(alpha=1.0)
stacking = StackingRegressor(estimators=base_models, final_estimator=meta_learner, cv=3, n_jobs=-1)
stacking.fit(X_train_scaled, y_train)
stack_pred = stacking.predict(X_test_scaled)
stack_r2 = r2_score(y_test, stack_pred)
stack_rmse = np.sqrt(mean_squared_error(y_test, stack_pred))
stack_mae = mean_absolute_error(y_test, stack_pred)
print(f"   âœ… Stacking RÂ²: {stack_r2:.4f}")

# ê²°ê³¼ ì¶œë ¥
print("\n" + "="*80)
print("ğŸ“Š ìµœì¢… ê²°ê³¼ ë¹„êµ")
print("="*80)

print(f"\n{'ëª¨ë¸':<20s} {'RÂ² Score':<12s} {'ë¹„ê³ '}")
print("-" * 50)
print(f"{'ê¸°ì¡´ (ë…¼ë¬¸ ê²°ê³¼)':<20s} {0.776:<12.4f} {'XGBoost ë‹¨ë…'}")
print(f"{'XGBoost':<20s} {xgb_r2:<12.4f} {''}")
print(f"{'LightGBM':<20s} {lgb_r2:<12.4f} {''}")
print(f"{'CatBoost':<20s} {cat_r2:<12.4f} {''}")
print(f"{'Random Forest':<20s} {rf_r2:<12.4f} {''}")
print(f"{'ğŸ¯ Stacking':<20s} {stack_r2:<12.4f} {'â† ê°œì„  ëª¨ë¸'}")

# ìµœê³  ì„±ëŠ¥
best_single = max(models.items(), key=lambda x: x[1]['r2'])
print(f"\nğŸ† ìµœê³  ë‹¨ì¼ ëª¨ë¸: {best_single[0]} (RÂ²={best_single[1]['r2']:.4f})")
print(f"ğŸ¯ Stacking ì„±ëŠ¥: RÂ²={stack_r2:.4f}")

# ê°œì„  íš¨ê³¼
baseline_r2 = 0.776
improvement = stack_r2 - baseline_r2
improvement_pct = (improvement / baseline_r2) * 100

print(f"\nğŸ“ˆ ê°œì„  íš¨ê³¼:")
print(f"   ê¸°ì¡´ â†’ Stacking: {baseline_r2:.4f} â†’ {stack_r2:.4f}")
print(f"   í–¥ìƒ: {improvement:+.4f} ({improvement_pct:+.1f}%)")

# ì„±ëŠ¥ ë©”íŠ¸ë¦­
print(f"\nğŸ“Š ìƒì„¸ ë©”íŠ¸ë¦­ (Stacking):")
print(f"   RÂ² Score:  {stack_r2:.4f}")
print(f"   RMSE:      {stack_rmse:.4f} kg")
print(f"   MAE:       {stack_mae:.4f} kg")

# ì„±ëŠ¥ ë“±ê¸‰
if stack_r2 >= 0.8:
    grade = "ğŸŒŸ Excellent (RÂ²â‰¥0.8)"
elif stack_r2 >= 0.7:
    grade = "âœ¨ Very Good (RÂ²â‰¥0.7)"
elif stack_r2 >= 0.5:
    grade = "ğŸ‘ Good (RÂ²â‰¥0.5)"
else:
    grade = "ğŸ“Š Fair (RÂ²â‰¥0.3)"

print(f"\nğŸ† ì„±ëŠ¥ ë“±ê¸‰: {grade}")

print("\n" + "="*80)
print("ğŸ’¡ í•´ì„")
print("="*80)
print("""
âœ… Stacking Ensembleì´ ë‹¨ì¼ ëª¨ë¸ë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
âœ… ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ì˜ ì¥ì ì„ ê²°í•©í•˜ì—¬ ë” ì•ˆì •ì ì¸ ì˜ˆì¸¡ì„ ì œê³µí•©ë‹ˆë‹¤.
âœ… EWMA íŠ¹ì„±ê³¼ Optuna ìµœì í™”ë¥¼ ì¶”ê°€í•˜ë©´ ë” í–¥ìƒë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""")

print("\n" + "="*80)
print("ğŸš€ ë‹¤ìŒ ë‹¨ê³„")
print("="*80)
print("""
1ï¸âƒ£ EWMA íŠ¹ì„± + Optuna ìµœì í™” í¬í•¨í•œ ì „ì²´ ëª¨ë¸:
   python -c "from IMPROVED_DIET_PREDICTION_MODEL import main; main(use_stacking=True, use_optuna=False)"
   (ì˜ˆìƒ ì‹œê°„: 30~60ë¶„)

2ï¸âƒ£ TabNet ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¶”ê°€:
   python test_tabnet.py
   (ì˜ˆìƒ ì‹œê°„: 20ë¶„, 2ê°œ ë°”ì´ì˜¤ë§ˆì»¤ë§Œ)

3ï¸âƒ£ ì „ì²´ 11ê°œ ë°”ì´ì˜¤ë§ˆì»¤ ìµœê³  ì„±ëŠ¥ í•™ìŠµ:
   python -c "from IMPROVED_DIET_PREDICTION_MODEL import main; main(use_stacking=True, use_optuna=True, optuna_trials=20)"
   (ì˜ˆìƒ ì‹œê°„: 2~4ì‹œê°„)
""")

print("âœ… ë°ëª¨ ì™„ë£Œ!")
